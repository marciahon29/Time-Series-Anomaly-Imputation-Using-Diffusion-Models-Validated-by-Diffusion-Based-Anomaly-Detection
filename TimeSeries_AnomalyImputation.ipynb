{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16y7w_ZjkGUVVLkVHwmjI-eXIk6lIcgnj","authorship_tag":"ABX9TyPSLjUbzbw45n6GuA/HmwVK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#### ANOMALY IMPUTATION USING DIFFUSION MODEL ####\n"],"metadata":{"id":"vz5FGGwsY08q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qlj_cPC_BbFt"},"outputs":[],"source":["import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","\n","# ------------------------------\n","# 0) MOUNT + PATHS\n","# ------------------------------\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","ANOM_FOLDER  = \"AnomalyDiffusion\"\n","BASE_IN_DIR  = \"/content/drive/MyDrive/Masters_IndependentStudy/Original_Datasets_Fridge\"\n","OUT_DIR      = f\"/content/drive/MyDrive/Masters_IndependentStudy/{ANOM_FOLDER}\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","TRAIN_RESIDENCE = \"REFIT_House01\"\n","TRAIN_CSV_PATH  = os.path.join(BASE_IN_DIR, f\"{TRAIN_RESIDENCE}_Fridge_15minutes.csv\")\n","\n","# ------------------------------\n","# 1) RESIDENCES + ANOMALY DATES\n","# ------------------------------\n","RESIDENCES = [\n","    \"REFIT_House01\",\n","    \"REFIT_House02\",\n","    \"REFIT_House03\",\n","    \"REFIT_House05\",\n","    \"REFIT_House07\",\n","    \"REFIT_House09\",\n","    \"REFIT_House15\",\n","    \"UKDALE_House01\",\n","    \"UKDALE_House02\",\n","    \"UKDALE_House05\",\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\",\n","    \"GREEND_House01\",\n","    \"GREEND_House03\",\n","]\n","\n","ANOMALY_DATES = {\n","    \"REFIT_House01\":  \"2015-03-09\",\n","    \"REFIT_House02\":  \"2015-01-26\",\n","    \"REFIT_House03\":  \"2015-02-02\",\n","    \"REFIT_House05\":  \"2015-03-02\",\n","    \"REFIT_House07\":  \"2015-03-09\",\n","    \"REFIT_House09\":  \"2015-03-23\",\n","    \"REFIT_House15\":  \"2015-03-16\",\n","    \"UKDALE_House01\": \"2016-06-06\",\n","    \"UKDALE_House02\": \"2013-09-16\",\n","    \"UKDALE_House05\": \"2014-10-20\",\n","    \"AMPds2_House01\": \"2013-11-11\",\n","    \"GREEND_House00\": \"2014-08-18\",\n","    \"GREEND_House01\": \"2014-09-15\",\n","    \"GREEND_House03\": \"2014-09-22\",\n","}\n","\n","SAVE_ANOM_TYPES = [\n","    \"stepchange\",\n","    \"multistepchange\",\n","    \"mirror\",\n","    \"repeating\",\n","    \"stuckmax\",\n","    \"stuckmin\",\n","    \"powercycling\",\n","]\n","\n","# ------------------------------\n","# 2) SETTINGS\n","# ------------------------------\n","NUM_STEPS   = 80\n","NUM_EPOCHS  = 50\n","BASE_CH     = 64\n","\n","K_NORMAL       = 220\n","PER_TYPE_HARD  = 520\n","PER_TYPE_EASY  = 320\n","\n","BATCH_SIZE  = 256\n","NUM_WORKERS = 2\n","\n","LAMBDA_X0   = 0.5\n","LAMBDA_GRAD = 0.55\n","\n","CFG_DROP_PROB  = 0.15\n","CFG_GUIDE_W    = 3.0\n","MIRROR_GUIDE_W = 3.0\n","\n","PRINT_EVERY_BATCHES = 25\n","\n","# Plot saving settings\n","PLOT_DPI_FULL = 200\n","PLOT_DPI_ZOOM = 220\n","\n","# ------------------------------\n","# 3) DEVICE\n","# ------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ------------------------------\n","# 4) CLASSES\n","# ------------------------------\n","ANOM_TYPES = {\n","    \"normal\":          0,\n","    \"stepchange\":      1,\n","    \"multistepchange\": 2,\n","    \"mirror\":          3,\n","    \"repeating\":       4,\n","    \"stuckmax\":        5,\n","    \"stuckmin\":        6,\n","    \"powercycling\":    7,\n","}\n","NUM_ANOM_TYPES = len(ANOM_TYPES)\n","HARD_TYPES = {\"stepchange\", \"multistepchange\", \"mirror\", \"repeating\"}\n","\n","# ------------------------------\n","# 5) COLUMN DETECTION + LOADING\n","# ------------------------------\n","def detect_columns(df: pd.DataFrame):\n","    # ts = timestamp, p = active_power\n","    ts_candidates = []\n","    for c in df.columns:\n","        cl = c.lower()\n","        if \"time\" in cl or \"date\" in cl or \"stamp\" in cl:\n","            ts_candidates.append(c)\n","\n","    p_candidates = []\n","    for c in df.columns:\n","        cl = c.lower()\n","        if \"active\" in cl and (\"power\" in cl or \"p_\" in cl or cl.endswith(\"p\")):\n","            p_candidates.append(c)\n","        elif cl in [\"active_power\", \"p\", \"p_active\"]:\n","            p_candidates.append(c)\n","\n","    ts_col = ts_candidates[0] if ts_candidates else df.columns[0]\n","    p_col  = p_candidates[0] if p_candidates else None\n","\n","    if p_col is None:\n","        numeric_cols = []\n","        for c in df.columns:\n","            if c == ts_col:\n","                continue\n","            if pd.api.types.is_numeric_dtype(df[c]):\n","                numeric_cols.append(c)\n","        if not numeric_cols:\n","            raise ValueError(\"Could not find a numeric power column. Rename your active power column to 'active_power'.\")\n","        p_col = numeric_cols[0]\n","\n","    return ts_col, p_col\n","\n","def load_two_cols(csv_path: str):\n","    df = pd.read_csv(csv_path)\n","    ts_col, p_col = detect_columns(df)\n","    df = df[[ts_col, p_col]].copy()\n","    df.rename(columns={ts_col: \"timestamp\", p_col: \"active_power\"}, inplace=True)\n","    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n","    df[\"active_power\"] = pd.to_numeric(df[\"active_power\"], errors=\"coerce\")\n","    df = df.dropna(subset=[\"timestamp\", \"active_power\"]).sort_values(\"timestamp\").reset_index(drop=True)\n","    return df, ts_col, p_col\n","\n","# ------------------------------\n","# 6) BUILD WINDOWS OF FULL DAYS BASED ON 15MIN INTERVALS (96/day)\n","# ------------------------------\n","def build_daily_windows(df: pd.DataFrame, expected_per_day=96):\n","    df2 = df.copy()\n","    df2[\"date\"] = df2[\"timestamp\"].dt.date\n","    days_raw, days_ts, day_keys = [], [], []\n","    for d, g in df2.groupby(\"date\"):\n","        g = g.sort_values(\"timestamp\")\n","        if len(g) != expected_per_day:\n","            continue\n","        days_raw.append(g[\"active_power\"].to_numpy(dtype=np.float32))\n","        days_ts.append(g[\"timestamp\"].to_numpy())\n","        day_keys.append(str(d))\n","    if len(days_raw) == 0:\n","        return None, None, None\n","    return np.stack(days_raw), np.stack(days_ts), day_keys\n","\n","# ------------------------------\n","# 7) SCALE EACH DAY TO [0,1] (Z-Normalization)\n","# ------------------------------\n","def scale_day_01(day: np.ndarray):\n","    dmin = float(np.min(day))\n","    dmax = float(np.max(day))\n","    if dmax - dmin < 1e-6:\n","        return np.zeros_like(day, dtype=np.float32), dmin, dmax\n","    return ((day - dmin) / (dmax - dmin)).astype(np.float32), dmin, dmax\n","\n","def scale_days_01(days_raw: np.ndarray):\n","    days_01 = []\n","    meta = []\n","    for d in days_raw:\n","        sd, dmin, dmax = scale_day_01(d)\n","        days_01.append(sd)\n","        meta.append((dmin, dmax))\n","    return np.stack(days_01).astype(np.float32), np.array(meta, dtype=np.float32)\n","\n","# ------------------------------\n","# 8) ANOMALY REGION\n","# - located at the middle third\n","# - add jitter\n","# ------------------------------\n","def anomaly_region_fixed(L=96):\n","    s = L // 3\n","    e = 2 * L // 3\n","    return s, e\n","\n","def anomaly_region_jitter(L=96, jitter=6, rng=None):\n","    if rng is None:\n","        rng = np.random.default_rng()\n","    base_s, base_e = anomaly_region_fixed(L)\n","    base_len = base_e - base_s\n","    ds = int(rng.integers(-jitter, jitter + 1))\n","    s = int(np.clip(base_s + ds, 0, L - 2))\n","    e = s + base_len\n","    if e > L:\n","        e = L\n","        s = e - base_len\n","    s = int(np.clip(s, 0, L - 2))\n","    e = int(np.clip(e, s + 2, L))\n","    return s, e\n","\n","# ------------------------------\n","# 9) TEMPLATE INJECTORS\n","# These are the anomalies: StepChange, MultiStepChange, Mirror, Repeating, StuckMAX, StuckMIN, PowerCycling.\n","# ------------------------------\n","def inject_stepchange(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    mid = int(rng.integers(s + 2, e - 2))\n","    low1 = float(rng.uniform(0.10, 0.35))\n","    high = float(rng.uniform(0.80, 0.98))\n","    out[s:mid] = np.clip(low1 + rng.uniform(-0.02, 0.02), 0.0, 1.0)\n","    out[mid:e] = np.clip(high + rng.uniform(-0.02, 0.02), 0.0, 1.0)\n","    return out.astype(np.float32), s, e\n","\n","def inject_multistepchange(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    seg = e - s\n","    cuts = np.sort(rng.choice(np.arange(1, seg-1), size=3, replace=False))\n","    edges = np.concatenate([[0], cuts, [seg]]).astype(int)\n","\n","    base = float(rng.uniform(0.08, 0.25))\n","    incs = rng.uniform(0.15, 0.28, size=3).astype(np.float32)\n","    levels = [base,\n","              base + incs[0],\n","              base + incs[0] + incs[1],\n","              base + incs[0] + incs[1] + incs[2]]\n","\n","    mx = max(levels)\n","    if mx > 0.98:\n","        scale = 0.98 / mx\n","        levels = [l * scale for l in levels]\n","    levels = [float(np.clip(l + rng.uniform(-0.015, 0.015), 0.0, 1.0)) for l in levels]\n","\n","    for k in range(4):\n","        a = s + edges[k]\n","        b = s + edges[k+1]\n","        out[a:b] = levels[k]\n","    return out.astype(np.float32), s, e\n","\n","def inject_mirror(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    seg = out[s:e].copy()\n","    out[s:e] = np.clip(seg[::-1], 0.0, 1.0)\n","    return out.astype(np.float32), s, e\n","\n","def inject_repeating(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    seg = e - s\n","    repeats = int(rng.integers(4, 8))\n","    edges = np.linspace(0, seg, repeats + 1).round().astype(int)\n","    center = float(rng.uniform(0.45, 0.55))\n","    amp    = float(rng.uniform(0.35, 0.48))\n","    phase  = float(rng.uniform(0, 2*np.pi))\n","    for i in range(repeats):\n","        a = edges[i]\n","        b = edges[i+1]\n","        n = b - a\n","        if n <= 0:\n","            continue\n","        t = np.linspace(0, 2*np.pi, n, endpoint=False).astype(np.float32)\n","        wave = np.sin(t + phase)\n","        out[s+a:s+b] = np.clip(center + amp * wave, 0.0, 1.0)\n","    return out.astype(np.float32), s, e\n","\n","def inject_stuckmax(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    out[s:e] = 1.0\n","    return out.astype(np.float32), s, e\n","\n","def inject_stuckmin(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    out[s:e] = 0.0\n","    return out.astype(np.float32), s, e\n","\n","def inject_powercycling(day01: np.ndarray, rng, s, e):\n","    out = day01.copy()\n","    seg = e - s\n","    high = float(rng.uniform(0.90, 1.00))\n","    low  = float(rng.uniform(0.00, 0.10))\n","    cycles = int(rng.integers(4, 11))\n","    period = max(2, seg // cycles)\n","    half   = max(1, int(period * rng.uniform(0.35, 0.65)))\n","    for i in range(seg):\n","        out[s + i] = high if ((i // half) % 2 == 0) else low\n","    return out.astype(np.float32), s, e\n","\n","INJECT_FUNCS = {\n","    \"stepchange\":      inject_stepchange,\n","    \"multistepchange\": inject_multistepchange,\n","    \"mirror\":          inject_mirror,\n","    \"repeating\":       inject_repeating,\n","    \"stuckmax\":        inject_stuckmax,\n","    \"stuckmin\":        inject_stuckmin,\n","    \"powercycling\":    inject_powercycling,\n","}\n","\n","# ------------------------------\n","# 10) DIFFUSION SCHEDULE (the Beta value)\n","# ------------------------------\n","def make_schedule(num_steps, device):\n","    betas  = torch.linspace(1e-4, 0.02, num_steps, device=device)\n","    alphas = 1.0 - betas\n","    abar   = torch.cumprod(alphas, dim=0)\n","    return betas, alphas, abar\n","\n","# ------------------------------\n","# 11) MODEL - ARCHITECTURE\n","# ------------------------------\n","# This function turns timestep into a smooth 64-D signal.\n","# This gives a sense of where it is in the noise-adding / noise-removing process.\n","# early â†’ lots of noise\n","# middle â†’ some noise\n","# late â†’ very little noise\n","def time_embed(t, dim=64):\n","    half = dim // 2\n","    freqs = torch.exp(-math.log(10000) / (half - 1) * torch.arange(half, device=t.device))\n","    args = t.float().unsqueeze(1) * freqs.unsqueeze(0)\n","    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n","\n","# We adopt a UNet1D based model that basically is encoders and decoders and skip connections\n","class UNet1D(nn.Module):\n","    def __init__(self, base=64, in_ch=2 + NUM_ANOM_TYPES):\n","        super().__init__()\n","        self.mlp = nn.Sequential(nn.Linear(64, base), nn.SiLU(), nn.Linear(base, base))\n","        self.c1  = nn.Conv1d(in_ch, base, 3, padding=1)\n","        self.c2  = nn.Conv1d(base, base*2, 3, padding=1)\n","        self.c3  = nn.Conv1d(base*2, base*4, 3, padding=1)\n","        self.d1  = nn.Conv1d(base*4, base*2, 3, padding=1)\n","        self.d2  = nn.Conv1d(base*2, base, 3, padding=1)\n","        self.out = nn.Conv1d(base, 1, 3, padding=1)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x, t):\n","        emb = self.mlp(time_embed(t)).unsqueeze(-1)\n","        h1 = self.act(self.c1(x)) + emb\n","        h2 = self.act(self.c2(h1))\n","        h3 = self.act(self.c3(h2))\n","        d1 = self.act(self.d1(h3))\n","        d2 = self.act(self.d2(d1 + h2))\n","        return self.out(d2 + h1)\n","\n","# ------------------------------\n","# 12) DATASET + UTILS\n","# ------------------------------\n","# Packages x, mask, condition maps.\n","class DiffDataset(Dataset):\n","    def __init__(self, X, M, C):\n","        self.X = X\n","        self.M = M\n","        self.C = C\n","    def __len__(self): return len(self.X)\n","    def __getitem__(self, i):\n","        x = torch.from_numpy(self.X[i]).float().unsqueeze(0)   # (1,96)\n","        m = torch.from_numpy(self.M[i]).float().unsqueeze(0)   # (1,96)\n","        c = torch.from_numpy(self.C[i]).float()                # (K,96)\n","        return x, m, c\n","\n","# mask for anomaly type\n","def make_c_map_mask_gated(cls_id: int, mask_1d: np.ndarray):\n","    c_map = np.zeros((NUM_ANOM_TYPES, 96), dtype=np.float32)\n","    c_map[cls_id, :] = mask_1d.astype(np.float32)\n","    return c_map\n","\n","# changes of signals within the masked anomaly region\n","# dx finds changes, mm filters them to anomaly-only change locations.\n","def masked_grad(x, m):\n","    dx = x[:, :, 1:] - x[:, :, :-1]\n","    mm = m[:, :, 1:] * m[:, :, :-1]\n","    return dx, mm\n","\n","# ------------------------------\n","# 13) HARD MIRROR CONSTRAINT - this is necessary because Mirror is not easily derived using the diffusion process.\n","# - Hard mirror constraint (non-diffusive anomaly)\n","# ------------------------------\n","@torch.no_grad()\n","def apply_hard_mirror(xt, x_clean01, s, e):\n","    xt[:, :, s:e] = torch.flip(x_clean01[:, :, s:e], dims=[-1])\n","    return xt\n","\n","# ------------------------------\n","# 14) INFERENCE: DDPM + Conditioning (Anomaly Classes)\n","# ------------------------------\n","@torch.no_grad()\n","def impute_localized_ddpm_cfg(model, x_clean01, cls_id, s, e, betas, alphas, abar, num_steps, guide_w=3.0):\n","    model.eval()\n","    B, C, T = x_clean01.shape\n","\n","    m = torch.zeros((B, 1, T), device=device, dtype=torch.float32)\n","    m[:, :, s:e] = 1.0\n","\n","    c_cond = torch.zeros((B, NUM_ANOM_TYPES, T), device=device, dtype=torch.float32)\n","    c_cond[:, cls_id, :] = m[:, 0, :]\n","    c_uncond = torch.zeros_like(c_cond)\n","\n","    # mirror is a special case that needs to be hardcoded\n","    if cls_id == ANOM_TYPES[\"mirror\"]:\n","        xt = x_clean01.clone()\n","        xt = apply_hard_mirror(xt, x_clean01, s, e)\n","        xt = xt * (1 - m) + (xt + 0.05 * torch.randn_like(xt)).clamp(0, 1) * m\n","    else:\n","        xt = x_clean01 * (1 - m) + torch.randn_like(x_clean01) * m\n","\n","    # Since it is INFERENCE, we start from the last timestep and return to the initial.\n","    for i in reversed(range(num_steps)):\n","        t = torch.full((B,), i, device=device, dtype=torch.long)\n","\n","        inp_c = torch.cat([xt, m, c_cond], dim=1)\n","        inp_u = torch.cat([xt, m, c_uncond], dim=1)\n","\n","        # Classifier-free guidance (conditional diffusion)\n","        # Trains the diffusion model to handle both conditional and unconditional (class or non-class)\n","        # It randomly drops the conditioning during training,\n","        # then at inference steers generation toward a desired anomaly\n","        # type by amplifying the difference between conditional and\n","        # unconditional noise predictions.\n","        eps_c = model(inp_c, t)\n","        eps_u = model(inp_u, t)\n","        eps = eps_u + guide_w * (eps_c - eps_u)\n","\n","        beta_t  = betas[i]\n","        alpha_t = alphas[i]\n","        abar_t  = abar[i]\n","\n","        # Reverse diffusion step (inference / imputation)\n","        mean = (1 / torch.sqrt(alpha_t)) * (xt - (beta_t / torch.sqrt(1 - abar_t + 1e-8)) * eps)\n","        if i > 0:\n","            z = torch.randn_like(xt)\n","            xt = mean + torch.sqrt(beta_t) * z\n","        else:\n","            xt = mean\n","\n","        # Locality constraint (if mirror then hard conditioning)\n","        xt = xt * m + x_clean01 * (1 - m)\n","        if cls_id == ANOM_TYPES[\"mirror\"]:\n","            xt = apply_hard_mirror(xt, x_clean01, s, e)\n","\n","    return xt\n","\n","# ------------------------------\n","# 15) PLOT SAVING\n","# ------------------------------\n","def save_plots(residence, anomaly_type, target_date, s, e,\n","               orig_raw, gen_raw, out_dir, folder_tag):\n","\n","    base = f\"{residence}_Fridge_{anomaly_type}_{folder_tag}\"\n","\n","    # -------- FULL DAY PLOT --------\n","    out_full = os.path.join(out_dir, f\"{base}_FULL.png\")\n","    plt.figure(figsize=(12, 4))\n","    plt.plot(orig_raw, label=\"original (raw units)\", alpha=0.75)\n","    plt.plot(gen_raw,  label=f\"generated {anomaly_type} (raw units)\", alpha=0.95)\n","    plt.axvspan(s, e, color=\"gray\", alpha=0.18)\n","    plt.title(f\"{base} | day={target_date}\")\n","    plt.legend(loc=\"best\")\n","    plt.tight_layout()\n","    plt.savefig(out_full, dpi=PLOT_DPI_FULL)\n","    plt.show()          # ðŸ‘ˆ DISPLAY\n","    plt.close()\n","\n","    # -------- ZOOM PLOT --------\n","    out_zoom = os.path.join(out_dir, f\"{base}_ZOOM.png\")\n","    plt.figure(figsize=(12, 3))\n","    plt.plot(orig_raw[s:e], label=\"original zoom\", alpha=0.75)\n","    plt.plot(gen_raw[s:e],  label=f\"{anomaly_type} zoom\", alpha=0.95)\n","    plt.title(f\"{base}_ZOOM | [{s}:{e}] | {target_date}\")\n","    plt.legend(loc=\"best\")\n","    plt.tight_layout()\n","    plt.savefig(out_zoom, dpi=PLOT_DPI_ZOOM)\n","    plt.show()          # ðŸ‘ˆ DISPLAY\n","    plt.close()\n","\n","    return out_full, out_zoom\n","\n","# ============================================================\n","# A) TRAIN ON REFIT_House01 ONLY - All other datasets use only this trained model.\n","# ============================================================\n","if not os.path.exists(TRAIN_CSV_PATH):\n","    raise FileNotFoundError(f\"Training file not found: {TRAIN_CSV_PATH}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"TRAINING ONLY ON:\", TRAIN_CSV_PATH)\n","print(\"=\"*80)\n","\n","train_df, ts_col, p_col = load_two_cols(TRAIN_CSV_PATH)\n","print(\"Detected columns -> timestamp:\", ts_col, \" | active_power:\", p_col)\n","print(\"Rows:\", len(train_df), \"Range:\", train_df[\"timestamp\"].min(), \"to\", train_df[\"timestamp\"].max())\n","\n","train_days_raw, _, _ = build_daily_windows(train_df, expected_per_day=96)\n","if train_days_raw is None or len(train_days_raw) < 10:\n","    raise ValueError(f\"Too few full 96-sample days found in {TRAIN_RESIDENCE}. Found: {0 if train_days_raw is None else len(train_days_raw)}\")\n","\n","train_days_01, _ = scale_days_01(train_days_raw)\n","print(\"Usable training days:\", len(train_days_01))\n","\n","# Build training set\n","rng = np.random.default_rng(0)\n","X_list, M_list, C_list = [], [], []\n","\n","# NORMAL - This code creates masked normal examples so the model learns that masked regions can contain no anomaly at all.\n","idxs = rng.choice(len(train_days_01), size=K_NORMAL, replace=True)\n","for idx in idxs:\n","    x = train_days_01[idx].copy()\n","    s_j, e_j = anomaly_region_jitter(96, jitter=6, rng=rng)\n","    mask = np.zeros(96, dtype=np.float32); mask[s_j:e_j] = 1.0\n","    X_list.append(x.astype(np.float32))\n","    M_list.append(mask)\n","    C_list.append(make_c_map_mask_gated(ANOM_TYPES[\"normal\"], mask))\n","\n","# ANOMALIES - generates training examples with actual injected anomalies, one anomaly type at a time\n","for name, cls_id in ANOM_TYPES.items():\n","    if name == \"normal\":\n","        continue\n","    k = PER_TYPE_HARD if name in HARD_TYPES else PER_TYPE_EASY\n","    idxs = rng.choice(len(train_days_01), size=k, replace=True)\n","    for idx in idxs:\n","        s_j, e_j = anomaly_region_jitter(96, jitter=6, rng=rng)\n","\n","        ### Anomalies are Inserted here\n","        inj, s_j, e_j = INJECT_FUNCS[name](train_days_01[idx], rng, s_j, e_j)\n","        mask = np.zeros(96, dtype=np.float32); mask[s_j:e_j] = 1.0\n","        X_list.append(inj.astype(np.float32))\n","        M_list.append(mask)\n","        C_list.append(make_c_map_mask_gated(cls_id, mask))\n","\n","X_np = np.stack(X_list).astype(np.float32)\n","M_np = np.stack(M_list).astype(np.float32)\n","C_np = np.stack(C_list).astype(np.float32)\n","print(\"Train shapes:\", X_np.shape, M_np.shape, C_np.shape)\n","\n","# Create the dataset paramters\n","dataset = DiffDataset(X_np, M_np, C_np)\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=(device.type == \"cuda\"),\n",")\n","\n","# Create the UNet1D architecture\n","model = UNet1D(base=BASE_CH).to(device)\n","opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n","betas, alphas, abar = make_schedule(NUM_STEPS, device=device)\n","\n","# Loop through epochs and calculate the losses\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    total = 0.0\n","    for bi, (x0, m, c_map) in enumerate(dataloader, start=1):\n","        x0 = x0.to(device, dtype=torch.float32)\n","        m  = m.to(device, dtype=torch.float32)\n","        c_map = c_map.to(device, dtype=torch.float32)\n","\n","        if CFG_DROP_PROB > 0:\n","            drop = (torch.rand((x0.size(0), 1, 1), device=device) < CFG_DROP_PROB).float()\n","            c_map = c_map * (1.0 - drop)\n","\n","        B = x0.size(0)\n","\n","        ### FORWARD DIFFUSION PROCESS\n","        t = torch.randint(0, NUM_STEPS, (B,), device=device, dtype=torch.long)\n","        eps = torch.randn_like(x0)\n","        abar_t = abar[t].view(B, 1, 1)\n","        xt = torch.sqrt(abar_t) * x0 + torch.sqrt(1 - abar_t) * eps\n","\n","        ### Noise prediction objective (core DDPM loss)\n","        inp = torch.cat([xt, m, c_map], dim=1)\n","        eps_pred = model(inp, t)\n","        loss_eps = (((eps_pred - eps) ** 2) * m).sum() / (m.sum() + 1e-8)\n","\n","        ### Clean-signal reconstruction from predicted noise\n","        x0_pred = (xt - torch.sqrt(1 - abar_t) * eps_pred) / (torch.sqrt(abar_t) + 1e-8)\n","\n","        ### Masked reconstruction loss (local fidelity)\n","        loss_x0 = (((x0_pred - x0) ** 2) * m).sum() / (m.sum() + 1e-8)\n","\n","        ### Gradient-consistency loss (shape realism)\n","        dxp, mm = masked_grad(x0_pred, m)\n","        dx0, _  = masked_grad(x0, m)\n","        loss_g = (torch.abs(dxp - dx0) * mm).sum() / (mm.sum() + 1e-8)\n","\n","        ### Total training objective (final equation)\n","        loss = loss_eps + LAMBDA_X0 * loss_x0 + LAMBDA_GRAD * loss_g\n","\n","        opt.zero_grad(set_to_none=True)\n","        loss.backward()\n","        opt.step()\n","\n","        total += loss.item()\n","\n","        if (bi % PRINT_EVERY_BATCHES) == 0 or bi == 1:\n","            print(f\"  [Train] Epoch {epoch+1}/{NUM_EPOCHS} | Batch {bi}/{len(dataloader)} | loss={loss.item():.6f}\")\n","\n","    print(f\"[Epoch Done] Epoch {epoch+1}/{NUM_EPOCHS} | avg_loss={total/len(dataloader):.6f}\")\n","\n","print(\"\\nTRAINING DONE. Starting inference on all residences...\")\n","\n","# ============================================================\n","# B) INFERENCE ON ALL RESIDENCES + SAVE CSV + SAVE PLOTS\n","# ============================================================\n","s_fix, e_fix = anomaly_region_fixed(96)\n","\n","# Loop through all residences\n","for residence in RESIDENCES:\n","    csv_path = os.path.join(BASE_IN_DIR, f\"{residence}_Fridge_15minutes.csv\")\n","    target_date = ANOMALY_DATES.get(residence, None)\n","\n","    print(\"\\n\" + \"-\"*80)\n","    print(\"INFER:\", residence, \"| date:\", target_date)\n","    print(\"-\"*80)\n","\n","    if target_date is None:\n","        print(\"[SKIP] Missing anomaly date.\")\n","        continue\n","    if not os.path.exists(csv_path):\n","        print(\"[SKIP] Missing file:\", csv_path)\n","        continue\n","\n","    df, ts_col, p_col = load_two_cols(csv_path)\n","    print(\"Detected columns -> timestamp:\", ts_col, \" | active_power:\", p_col)\n","    print(\"Rows:\", len(df), \"Range:\", df[\"timestamp\"].min(), \"to\", df[\"timestamp\"].max())\n","\n","    # Keep a full copy for \"save all timestamps\"\n","    df_full = df.copy()\n","    df_full[\"date_str\"] = df_full[\"timestamp\"].dt.date.astype(str)\n","\n","    days_raw, days_ts, day_keys = build_daily_windows(df, expected_per_day=96)\n","    if days_raw is None or len(days_raw) < 1:\n","        print(\"[SKIP] No full 96-sample days found.\")\n","        continue\n","\n","    if target_date not in day_keys:\n","        print(\"[SKIP] Target day not found as a complete 96-sample day.\")\n","        continue\n","\n","    infer_idx = day_keys.index(target_date)\n","    infer_ts = days_ts[infer_idx]\n","    infer_day_raw = days_raw[infer_idx]\n","\n","    infer_day_01, dmin, dmax = scale_day_01(infer_day_raw)\n","    x_clean01 = torch.tensor(infer_day_01, dtype=torch.float32, device=device).view(1, 1, 96)\n","\n","    print(f\"[OK] Using fixed mask [{s_fix}:{e_fix}] and saving to {OUT_DIR}\")\n","\n","    # For each anomaly type, run the trained model\n","    for anomaly_type in SAVE_ANOM_TYPES:\n","        cls_id = ANOM_TYPES[anomaly_type]\n","        w = MIRROR_GUIDE_W if anomaly_type == \"mirror\" else CFG_GUIDE_W\n","\n","        gen01 = impute_localized_ddpm_cfg(\n","            model=model,\n","            x_clean01=x_clean01,\n","            cls_id=cls_id,\n","            s=s_fix, e=e_fix,\n","            betas=betas, alphas=alphas, abar=abar,\n","            num_steps=NUM_STEPS,\n","            guide_w=w\n","        ).detach().cpu().numpy().reshape(-1)\n","\n","        gen01 = np.clip(gen01, 0.0, 1.0)\n","        gen = gen01 * (dmax - dmin) + dmin  # unscale to raw units\n","\n","        # ---- SAVE CSV (ALL timestamps; modify ONLY the anomaly day) ----\n","        df_out = df_full.copy()\n","        mask_day = (df_out[\"date_str\"] == target_date)\n","\n","        if mask_day.sum() != 96:\n","            print(f\"  [WARN] {residence} {anomaly_type}: expected 96 rows on {target_date}, found {mask_day.sum()}\")\n","\n","        # write the generated anomaly day back into the full series\n","        df_out.loc[mask_day, \"active_power\"] = gen.astype(np.float32)\n","\n","        out_csv = os.path.join(OUT_DIR, f\"{residence}_Fridge_{anomaly_type}_{ANOM_FOLDER}.csv\")\n","        df_out[[\"timestamp\", \"active_power\"]].to_csv(out_csv, index=False)\n","\n","        # ---- SAVE PLOTS (FULL + ZOOM) ----\n","        out_full, out_zoom = save_plots(\n","            residence=residence,\n","            anomaly_type=anomaly_type,\n","            target_date=target_date,\n","            s=s_fix, e=e_fix,\n","            orig_raw=infer_day_raw,\n","            gen_raw=gen,\n","            out_dir=OUT_DIR,\n","            folder_tag=ANOM_FOLDER\n","        )\n","\n","        print(f\"  [SAVED] {residence} | {anomaly_type}\")\n","        print(f\"    CSV : {out_csv}\")\n","        print(f\"    FULL: {out_full}\")\n","        print(f\"    ZOOM: {out_zoom}\")\n","\n","print(\"\\nALL DONE.\")\n","print(\"Saved folder:\", OUT_DIR)\n"]}]}