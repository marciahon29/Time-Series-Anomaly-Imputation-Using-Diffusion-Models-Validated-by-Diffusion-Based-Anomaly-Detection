{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16y7w_ZjkGUVVLkVHwmjI-eXIk6lIcgnj","authorship_tag":"ABX9TyNK6gzMFZ4UxDjYZG/dn2IA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#### ANOMALY DETECTION WITH DIFFUSION ####"],"metadata":{"id":"vz5FGGwsY08q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# IMPORTS\n","# ============================================================\n","import os\n","import time\n","import psutil\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.fft import fft\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    confusion_matrix, roc_auc_score, average_precision_score\n",")\n","\n","# ============================================================\n","# LOGGER\n","# ============================================================\n","def log(msg):\n","    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n","\n","# ============================================================\n","# CONFIGURATION\n","# ============================================================\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","WINDOW_SIZE = 24\n","BATCH_SIZE = 64\n","EPOCHS = 30\n","LR = 1e-4\n","\n","ANOMALY_IMPUTATION_TYPE = \"AnomalyDiffusion\"\n","BASE_INPUT = f\"/content/drive/MyDrive/Masters_IndependentStudy/{ANOMALY_IMPUTATION_TYPE}_labelled\"\n","BASE_OUTPUT = f\"/content/drive/MyDrive/Masters_IndependentStudy/{ANOMALY_IMPUTATION_TYPE}_prediction\"\n","STATS_FILE = f\"/content/drive/MyDrive/Masters_IndependentStudy/{ANOMALY_IMPUTATION_TYPE}_Statistics.csv\"\n","\n","RESIDENCES = [\n","    \"AMPds2_House01\",\n","    \"GREEND_House00\", \"GREEND_House01\", \"GREEND_House03\",\n","    \"UKDALE_House01\", \"UKDALE_House02\", \"UKDALE_House05\",\n","    \"REFIT_House01\", \"REFIT_House02\", \"REFIT_House03\", \"REFIT_House05\",\n","    \"REFIT_House07\", \"REFIT_House09\", \"REFIT_House15\"\n","]\n","\n","ANOMALY_TYPES = [\n","    \"stepchange\", \"multistepchange\", \"mirror\",\n","    \"repeating\", \"stuckmax\", \"stuckmin\", \"powercycling\"\n","]\n","\n","THRESHOLDS = [70] #[70, 75, 80, 85, 90]  # Percentiles to loop through\n","\n","# ============================================================\n","# DATASET AND WINDOWS\n","# ============================================================\n","class PowerDataset(Dataset):\n","    def __init__(self, series):\n","        self.series = series\n","    def __len__(self):\n","        return len(self.series)\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.series[idx], dtype=torch.float32).unsqueeze(0)\n","\n","def create_windows(signal, window=WINDOW_SIZE):\n","    return np.array([signal[i:i + window] for i in range(0, len(signal) - window + 1, window)])\n","\n","# ============================================================\n","# SIMPLE UNET1D\n","# ============================================================\n","class UNet1D(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.enc = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n","        self.dec = nn.Conv1d(16, 1, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        e = self.relu(self.enc(x))\n","        d = self.dec(e)\n","        return d + x  # skip connection\n","\n","# ============================================================\n","# DDPM SCHEDULER\n","# ============================================================\n","class DDPM:\n","    def __init__(self, T=1000):\n","        self.T = T\n","        self.beta = torch.linspace(1e-4, 0.02, T).to(DEVICE)\n","        self.alpha = 1.0 - self.beta\n","        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n","    def add_noise(self, x0, t):\n","        eps = torch.randn_like(x0)\n","        a_bar = self.alpha_bar[t].view(-1, 1, 1)\n","\n","        #### FUNDAMENTAL EQUATION - FORWARD PASS ####\n","        xt = torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * eps\n","        return xt, eps\n","\n","# ============================================================\n","# TRAINING\n","# ============================================================\n","def train_model(train_loader):\n","    log(\"Initializing UNet1D diffusion model\")\n","    model = UNet1D().to(DEVICE)\n","    ddpm = DDPM()\n","    opt = torch.optim.Adam(model.parameters(), lr=LR)\n","    loss_fn = nn.MSELoss()\n","\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(EPOCHS):\n","        log(f\"Epoch {epoch + 1}/{EPOCHS} started\")\n","        for x in train_loader:\n","            x = x.to(DEVICE)\n","            t = torch.randint(0, ddpm.T, (x.size(0),)).to(DEVICE)\n","            xt, eps = ddpm.add_noise(x, t)\n","            eps_hat = model(xt)\n","            loss = loss_fn(eps_hat, eps)\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","        log(f\"Epoch {epoch + 1} completed\")\n","    elapsed = time.time() - start_time\n","    log(f\"Training finished in {elapsed:.2f} seconds\")\n","    return model, elapsed\n","\n","# ============================================================\n","# ANOMALY SCORE COMPUTATION\n","# ============================================================\n","def compute_scores(x, x_hat):\n","    mae_t = np.mean(np.abs(x - x_hat), axis=1)\n","    rx = np.log1p(np.abs(fft(x)))\n","    rx_hat = np.log1p(np.abs(fft(x_hat)))\n","    mae_f = np.mean(np.abs(rx - rx_hat), axis=1)\n","    def z_robust(v):\n","        med = np.median(v)\n","        mad = np.median(np.abs(v - med)) + 1e-6\n","        return (v - med) / mad\n","\n","    #### FUNDAMENTAL EQUATION - ANOMALY SCORE ####\n","    score = 0.5 * (z_robust(mae_t) + z_robust(mae_f))\n","    return score, mae_t, mae_f\n","\n","# ============================================================\n","# METRICS HELPERS\n","# ============================================================\n","def cohen_d(x1, x2):\n","    n1, n2 = len(x1), len(x2)\n","    s1, s2 = np.var(x1, ddof=1), np.var(x2, ddof=1)\n","    pooled = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / (n1 + n2 -2))\n","    if pooled == 0: return 0.0\n","    return (np.mean(x1) - np.mean(x2)) / pooled\n","\n","def separation_delta(x1, x2):\n","    return np.mean(x1) - np.mean(x2)\n","\n","# ============================================================\n","# MAIN PIPELINE - TRAINING, COMPUTE SCORES, AND INFERENCE\n","# ============================================================\n","log(\"Starting diffusion anomaly detection pipeline\")\n","stats = []\n","\n","for residence in RESIDENCES:\n","    log(f\"Processing residence: {residence}\")\n","\n","    # ------------------ TRAINING ---------------------\n","    train_path = f\"{BASE_INPUT}/{residence}_Fridge_stepchange_{ANOMALY_IMPUTATION_TYPE}_labelled.csv\"\n","    log(f\"Loading training data: {train_path}\")\n","    df_train = pd.read_csv(train_path)\n","    signal_train = df_train[\"active_power\"].values\n","    windows_train = create_windows(signal_train)\n","    split = int(0.8 * len(windows_train))\n","    train_windows = windows_train[:split]\n","    train_ds = PowerDataset(train_windows)\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    train_peak_mb = psutil.Process().memory_info().rss / (1024 ** 2)\n","    model, train_time = train_model(train_loader)\n","\n","    # ------------------ COMPUTE SCORES ---------------------\n","    model.eval()\n","    with torch.no_grad():\n","        recon_train = model(torch.tensor(train_windows).float().unsqueeze(1).to(DEVICE)).cpu().numpy().squeeze()\n","    scores_train, _, _ = compute_scores(train_windows, recon_train)\n","    varthreshold = np.var(scores_train)\n","\n","    # ------------------ INFERENCE ---------------------\n","    for anomaly_type in ANOMALY_TYPES:\n","        log(f\"Inference for anomaly type: {anomaly_type}\")\n","        path = f\"{BASE_INPUT}/{residence}_Fridge_{anomaly_type}_{ANOMALY_IMPUTATION_TYPE}_labelled.csv\"\n","        df_infer = pd.read_csv(path)\n","\n","        # Normalize ground truth\n","        df_infer[\"anomaly_groundtruth\"] = df_infer[\"anomaly_groundtruth\"].astype(str).str.strip().str.lower()\n","\n","        # Actual counts\n","        ActualNormal = np.sum(df_infer[\"anomaly_groundtruth\"] == \"normal\")\n","        ActualAnomaly = np.sum(df_infer[\"anomaly_groundtruth\"] == \"anomaly\")\n","\n","        # Create windows\n","        windows_infer = create_windows(df_infer[\"active_power\"].values)\n","        start_inf = time.time()\n","        with torch.no_grad():\n","            recon_infer = model(torch.tensor(windows_infer).float().unsqueeze(1).to(DEVICE)).cpu().numpy().squeeze()\n","\n","        scores_windows, _, _ = compute_scores(windows_infer, recon_infer)\n","\n","        # Propagate scores to rows\n","        scores_per_row = np.zeros(len(df_infer))\n","        for i, score in enumerate(scores_windows):\n","            start_idx = i * WINDOW_SIZE\n","            end_idx = start_idx + WINDOW_SIZE\n","            scores_per_row[start_idx:end_idx] = score\n","        scores_per_row = scores_per_row[:len(df_infer)]\n","\n","        # ------------------ LOOP THROUGH THRESHOLDS ---------------------\n","        # For testing purposes we loop through many thresholds\n","        # - During inference, only one threshold is mentioned in the variable \"THRESHOLDS\"\n","        for THRESHOLD_PCT in THRESHOLDS:\n","            threshold = np.percentile(scores_train, THRESHOLD_PCT)\n","            preds = np.where(scores_per_row > threshold, \"anomaly\", \"normal\")\n","            df_out = df_infer.copy()\n","            df_out[\"anomaly_prediction\"] = preds\n","            os.makedirs(BASE_OUTPUT, exist_ok=True)\n","            out_path = f\"{BASE_OUTPUT}/{residence}_Fridge_{anomaly_type}_{ANOMALY_IMPUTATION_TYPE}_prediction_{THRESHOLD_PCT}.csv\"\n","            df_out.to_csv(out_path, index=False)\n","            log(f\"Predictions saved: {out_path}\")\n","\n","            # Metrics\n","            y_true = np.where(df_out[\"anomaly_groundtruth\"] == \"anomaly\", 1, 0)\n","            y_pred = (preds == \"anomaly\").astype(int)\n","            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","            normal_pct = 100 * np.sum(y_pred == 0) / len(y_pred)\n","            anomaly_pct = 100 * np.sum(y_pred == 1) / len(y_pred)\n","            inf_peak_mb = psutil.Process().memory_info().rss / (1024 ** 2)\n","\n","            # ROC AUC and PR AUC on thresholded predictions\n","            if len(np.unique(y_pred)) < 2:\n","                roc_auc = 0.0\n","                pr_auc = 0.0\n","            else:\n","                roc_auc = roc_auc_score(y_true, y_pred)\n","                pr_auc = average_precision_score(y_true, y_pred)\n","\n","            # Cohen's d and separation delta on thresholded predictions\n","            normal_scores = y_pred[y_true==0]\n","            anomaly_scores = y_pred[y_true==1]\n","            if len(normal_scores) > 0 and len(anomaly_scores) > 0:\n","                d_cohen = cohen_d(normal_scores, anomaly_scores)\n","                separation = separation_delta(normal_scores, anomaly_scores)\n","            else:\n","                d_cohen = 0.0\n","                separation = 0.0\n","\n","            stats.append({\n","                \"residence\": residence,\n","                \"appliance\": \"Fridge\",\n","                \"anomaly type\": anomaly_type,\n","                \"thresholdpct\": THRESHOLD_PCT,\n","                \"thresholdvalue\": threshold,\n","                \"varthreshold\": varthreshold,\n","                \"windows\": len(windows_infer),\n","                \"trainingtimesec\": train_time,\n","                \"inferencetimesec\": time.time() - start_inf,\n","                \"trainpeakmb\": train_peak_mb,\n","                \"inferencepeakmb\": inf_peak_mb,\n","                \"accuracy\": accuracy_score(y_true, y_pred),\n","                \"precision\": precision_score(y_true, y_pred, zero_division=0),\n","                \"recall\": recall_score(y_true, y_pred, zero_division=0),\n","                \"f1_score\": f1_score(y_true, y_pred, zero_division=0),\n","                \"normal_%\": normal_pct,\n","                \"anomaly_%\": anomaly_pct,\n","                \"total\": len(y_true),\n","                \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n","                \"ActualNormal\": ActualNormal,\n","                \"ActualAnomaly\": ActualAnomaly,\n","                \"roc_auc\": roc_auc,\n","                \"pr_auc\": pr_auc,\n","                \"cohens_d\": d_cohen,\n","                \"separation_delta\": separation\n","            })\n","\n","# ============================================================\n","# SAVE FINAL STATISTICS\n","# ============================================================\n","log(\"Saving statistics CSV\")\n","pd.DataFrame(stats).to_csv(STATS_FILE, index=False)\n","log(\"âœ… Pipeline completed successfully\")\n"],"metadata":{"id":"iAossgoqCHqQ"},"execution_count":null,"outputs":[]}]}