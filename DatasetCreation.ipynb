{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1iwg8bozvxvsuNaILhs0365ESYeMZqn2Q","authorship_tag":"ABX9TyPeCb0UV6/XCCVOk8+KQ7Ql"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### OUTLINE OF FILE #####\n","###\n","### Basically, this code creates all the datasets to be used in DANDY.\n","###\n","### This files processes the original data (REFT/UKDALE/Ampds2/GREEND)\n","### It creates the Centroids file\n","### It also creates the anomalies and merges to get the final file\n","### This final file has the following:\n","### 'timestamp', 'active_power', 'ground_truth_anomaly', 'ground_truth_appliance'\n","###\n","###\n","##### SECTIONS OF CODE IN THIS FILE #####\n","###\n","#### REFIT DATASET - Create Dataset and combine for 15 minutes ####\n","#### UKDALE DATASET - Create Dataset and combine for 15 minutes ####\n","#### AMPds2 Dataset - Create Dataset and combine for 15 minutes ####\n","#### GREEND Dataset - Create Dataset and combine for 15 minutes ####\n","###\n","#### CENTROIDS  - Average Power Consumption for appliances within each dataset-house combination ####\n","#### ANOMALIES - Create anomalies (7 types for 7 days) plus ground_truth_anomaly ####\n","#### MERGE - combines anomalies and normal data plus sets ground_truth_appliance ####\n","###\n","###"],"metadata":{"id":"0kyMTPnl09JW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sunlfgly09D3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUlWa3Kkb_3q"},"outputs":[],"source":["#### REFIT DATASET - Create Dataset and combine for 15 minutes ####\n","#\n","# REFIT Dataset: https://pureportal.strath.ac.uk/en/datasets/refit-electrical-load-measurements-cleaned/\n","#\n","# The dataset is from : /content/drive/MyDrive/Paper02_14Datasets/REFIT_ORIGINALS\n","# The files are: {residence}.csv\n","# residence is: \"House02', \"House03\", \"House08\", \"House09\"\n","# appliance is: \"Fridge\", \"WashingMachine\", \"Microwave\"\n","#\n","# For residence \"House02\", Fridge = \"1\", WashingMachine = \"2\", Microwave = \"5\"\n","# For residence \"House03\", Fridge = \"2\", WashingMachine = \"6\", Microwave = \"8\"\n","# For residence \"House08\", Fridge = \"1\", WashingMachine = \"4\", Microwave = \"8\"\n","# For residence \"House09\", Fridge = \"1\", WashingMachine = \"3\", Microwave = \"6\"\n","#\n","# Create files:\n","# /content/drive/MyDrive/Paper02_14Datasets/REFIT_ORIGINALS/REFIT_{residence}_{appliance}.csv\n","#\n","# For appliance each appliance, please extract from {residence}.csv the columns 'Time\" and \"Appliance{appliance}\"\n","# Rename \"Time\" to \"timestamp\", and \"Appliance{WashingMachine}\" to \"active_power\"\n","# Save to: /content/drive/MyDrive/Paper02_14Datasets/REFIT_ORIGINALS/REFIT_{residence}_{appliance}.csv\n","#\n","# Aggregate \"active_power\" for every 1 minute.\n","# Get the distribution of this \"active_power\" and zero the bottom 90th percentile.\n","# Next, please add the \"active_power\" for every 15 minutes.\n","#\n","# Save to: /content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/REFIT_{residence}_{appliance}_15minutes.csv\n","#\n","# For each {residence}: please print the date range: \"First Date\" and the \"Last Date\"\n","\n","import os\n","import pandas as pd\n","\n","# ---------------------------\n","# Paths\n","# ---------------------------\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets/REFIT_ORIGINALS\"\n","OUT_15 = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES\"\n","os.makedirs(OUT_15, exist_ok=True)\n","\n","# ---------------------------\n","# Residence → Appliance channel IDs\n","# ---------------------------\n","house_appliances = {\n","    \"House01\": {\"Fridge\": \"1\", \"WashingMachine\": \"5\", \"Dishwasher\": \"6\"},\n","    \"House02\": {\"Fridge\": \"1\", \"WashingMachine\": \"2\", \"Dishwasher\": \"3\"},\n","    \"House03\": {\"Fridge\": \"2\", \"WashingMachine\": \"6\", \"Dishwasher\": \"5\"},\n","    \"House05\": {\"Fridge\": \"1\", \"WashingMachine\": \"3\", \"Dishwasher\": \"4\"},\n","\t  \"House07\": {\"Fridge\": \"1\", \"WashingMachine\": \"5\", \"Dishwasher\": \"6\"},\n","\t  \"House09\": {\"Fridge\": \"1\", \"WashingMachine\": \"3\", \"Dishwasher\": \"4\"},\n","\t  \"House15\": {\"Fridge\": \"1\", \"WashingMachine\": \"3\", \"Dishwasher\": \"4\"},\n","}\n","\n","# ---------------------------\n","# Tries two date formats, keeps the one that works.\n","#\n","# Formats are:\n","# Month-first (default) → MM/DD/YYYY (e.g., 12/31/2023)\n","# Day-first → DD/MM/YYYY (e.g., 31/12/2023)\n","# ---------------------------\n","def parse_time(series):\n","    \"\"\"Robust timestamp parsing: try default, then dayfirst=True if needed.\"\"\"\n","    ts = pd.to_datetime(series, errors=\"coerce\", utc=False)\n","    if ts.isna().any():\n","        ts2 = pd.to_datetime(series, errors=\"coerce\", dayfirst=True, utc=False)\n","        if ts2.notna().sum() > ts.notna().sum():\n","            ts = ts2\n","    return ts\n","\n","# ---------------------------\n","# Loops one residence at a time.\n","# Safely loads house data, parses timestamps, prepares date tracking.\n","# Then processes data.\n","# ---------------------------\n","for residence, appmap in house_appliances.items():\n","    # Load once per residence\n","    input_csv = f\"{BASE}/{residence}.csv\"\n","    if not os.path.exists(input_csv):\n","        print(f\"[WARN] Missing file: {input_csv}\")\n","        continue\n","    df_src = pd.read_csv(input_csv)\n","\n","    # Parse Time → timestamp\n","    if \"Time\" not in df_src.columns:\n","        print(f\"[ERROR] 'Time' column not found in {input_csv}. Columns: {list(df_src.columns)[:10]}...\")\n","        continue\n","    df_src[\"timestamp\"] = parse_time(df_src[\"Time\"])\n","    if df_src[\"timestamp\"].isna().all():\n","        print(f\"[ERROR] Could not parse any timestamps in {input_csv}.\")\n","        continue\n","\n","    # Track per-residence date range across appliances\n","    residence_first = None\n","    residence_last = None\n","\n","    # ---------------------------\n","    # For each appliance in the residence, determine the 15 minute active power\n","    # ---------------------------\n","    for appliance, app_id in appmap.items():\n","        col = f\"Appliance{app_id}\"\n","        if col not in df_src.columns:\n","            print(f\"[WARN] {residence} - {appliance}: column '{col}' not found. Skipping.\")\n","            continue\n","\n","        # ---------------------------\n","        # 1) Extract & Save per-appliance ORIGINAL\n","        # ---------------------------\n","        df_ap = df_src[[\"timestamp\", col]].copy()\n","        # Rename selected Appliance{id} column to active_power\n","        df_ap.rename(columns={col: \"active_power\"}, inplace=True)\n","\n","        # Clean & sort\n","        df_ap = df_ap.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n","        df_ap[\"active_power\"] = pd.to_numeric(df_ap[\"active_power\"], errors=\"coerce\").fillna(0)\n","\n","        out_appliance_csv = f\"{BASE}/REFIT_{residence}_{appliance}.csv\"\n","        df_ap.to_csv(out_appliance_csv, index=False)\n","\n","        # ---------------------------\n","        # 2) 1-minute aggregation (sum)\n","        # ---------------------------\n","        df_min = df_ap.set_index(\"timestamp\").sort_index()\n","        df_1min = df_min.resample(\"1min\").sum(numeric_only=True)\n","\n","        # ---------------------------\n","        # 3) Zero bottom 90% by value (values < 90th percentile → 0)\n","        # ---------------------------\n","        if df_1min.empty:\n","            print(f\"[WARN] {residence} - {appliance}: No data after 1-minute resample.\")\n","            continue\n","\n","        p90 = df_1min[\"active_power\"].quantile(0.9)\n","        df_1min[\"active_power\"] = df_1min[\"active_power\"].where(df_1min[\"active_power\"] >= p90, 0)\n","\n","        # ---------------------------\n","        # 4) 15-minute aggregation (sum)\n","        # ---------------------------\n","        df_15 = df_1min.resample(\"15min\").sum(numeric_only=True)\n","\n","        # ---------------------------\n","        # 5) Save 15-minute file\n","        # ---------------------------\n","        out_15_csv = f\"{OUT_15}/REFIT_{residence}_{appliance}_15minutes.csv\"\n","        df_15.to_csv(out_15_csv, index_label=\"timestamp\")\n","\n","        # Update per-residence date range\n","        if not df_15.empty:\n","            first_dt = df_15.index.min()\n","            last_dt  = df_15.index.max()\n","            residence_first = first_dt if residence_first is None else min(residence_first, first_dt)\n","            residence_last  = last_dt  if residence_last  is None else max(residence_last,  last_dt)\n","\n","    # ---------------------------\n","    # 6) Print per-residence date range summary\n","    # ---------------------------\n","    if residence_first is None or residence_last is None:\n","        print(f\"{residence}: First Date = N/A, Last Date = N/A (no 15-minute files written)\")\n","    else:\n","        print(f\"{residence}: First Date = {residence_first}, Last Date = {residence_last}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"XcMN0-901tEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### UKDALE DATASET - Create Dataset and combine for 15 minutes ####\n","#\n","# UKDALE dataset: https://ukerc.rl.ac.uk/cgi-bin/dataDiscover.pl?Action=detail&dataid=7d78f943-f9fe-413b-af52-1816f9d968b0\n","#\n","# The dataset is from : /content/drive/MyDrive/Paper02_14Datasets/UKDALE_ORIGINALS\n","# residence is: \"House01', \"House02\", \"House05\"\n","# appliance is: \"Fridge\", \"WashingMachine\", \"Microwave\"\n","# The files are: UKDALE_{residence}_{appliance}.dat\n","#\n","# For these files, the first column is \"timestamp\" and the second is \"active_power\"\n","#\n","# Aggregate \"active_power\" for every 1 minute.\n","# Get the distribution of this \"active_power\" and zero the bottom 90th percentile.\n","# Next, please add the \"active_power\" for every 15 minutes.\n","#\n","# Save to: /content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/UKDALE_{residence}_{appliance}_15minutes.csv\n","#\n","# For each {residence}: please print the date range: \"First Date\" and the \"Last Date\"\n","\n","import os\n","import pandas as pd\n","\n","# ----------------------------\n","# Paths & Config\n","# ----------------------------\n","input_dir = \"/content/drive/MyDrive/Paper02_14Datasets/UKDALE_ORIGINALS\"\n","output_dir = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","residences = [\"House01\", \"House02\", \"House05\"]\n","appliances = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","\n","# Track overall first/last timestamps per residence\n","residence_ranges = {r: {\"first\": None, \"last\": None} for r in residences}\n","\n","# ----------------------------\n","# Helper to update residence range\n","# ----------------------------\n","def _update_range(residence, idx):\n","    if idx.size == 0:\n","        return\n","    first, last = idx.min(), idx.max()\n","    current = residence_ranges[residence]\n","    if current[\"first\"] is None or first < current[\"first\"]:\n","        current[\"first\"] = first\n","    if current[\"last\"] is None or last > current[\"last\"]:\n","        current[\"last\"] = last\n","\n","# ----------------------------\n","# Processing Loop -  per residence and per appliance\n","# ----------------------------\n","for residence in residences:\n","    for appliance in appliances:\n","        filename = f\"UKDALE_{residence}_{appliance}.dat\"\n","        filepath = os.path.join(input_dir, filename)\n","\n","        if not os.path.exists(filepath):\n","            print(f\"❌ File not found: {filepath}\")\n","            continue\n","\n","        # Load 2-column whitespace-delimited (timestamp, active_power)\n","        # If there are header lines, set header=None; UK-DALE .dat typically has no header.\n","        df = pd.read_csv(\n","            filepath,\n","            sep=r\"\\s+\",\n","            header=None,\n","            names=[\"timestamp\", \"active_power\"],\n","            dtype={\"timestamp\": \"int64\", \"active_power\": \"float64\"},\n","            engine=\"python\",\n","        )\n","\n","        # Convert UNIX seconds to datetime index; sort & combine any duplicate timestamps\n","        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n","        df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n","        # If duplicate timestamps exist, sum them before resampling\n","        df = df.groupby(level=0)[\"active_power\"].sum().to_frame()\n","\n","        # ----------------------------\n","        # For each appliance in the residence, determine the 15 minute active power\n","        # ----------------------------\n","        # 1) Resample to 1-minute sums\n","        df_1min = df.resample(\"1min\").sum()\n","\n","        # 2) Zero the bottom 90th percentile (keep top 10% by value)\n","        threshold = df_1min[\"active_power\"].quantile(0.9)\n","        df_1min.loc[df_1min[\"active_power\"] < threshold, \"active_power\"] = 0.0\n","\n","        # 3) Resample to 15-minute sums\n","        df_15min = df_1min.resample(\"15min\").sum()\n","\n","        # Save to CSV with timestamp as first column\n","        outname = f\"UKDALE_{residence}_{appliance}_15minutes.csv\"\n","        outpath = os.path.join(output_dir, outname)\n","        df_15min.to_csv(outpath, index_label=\"timestamp\")\n","\n","        # Update the overall residence date range\n","        _update_range(residence, df_15min.index)\n","\n","        print(f\"✅ Saved: {outpath}  |  Rows: {len(df_15min)}\")\n","\n","# ----------------------------\n","# Print overall date ranges per residence\n","# ----------------------------\n","print(\"\\n=== Date Ranges by Residence (across all three appliances) ===\")\n","for residence in residences:\n","    first = residence_ranges[residence][\"first\"]\n","    last = residence_ranges[residence][\"last\"]\n","    if first is None or last is None:\n","        print(f\"{residence}: First Date = N/A, Last Date = N/A (no files found)\")\n","    else:\n","        # Display as ISO timestamps\n","        print(f\"{residence}: First Date = {first}, Last Date = {last}\")\n"],"metadata":{"id":"eGLFVJjU1s-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2fa_zl3m_lW9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### AMPds2 Dataset - Create Dataset and combine for 15 minutes ####\n","#\n","# AMPds2 dataset: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FIE0S4\n","#\n","# The dataset is from : /content/drive/MyDrive/Paper02_14Datasets/AMPds2_ORIGINALS\n","# residence is: \"House01\"\n","# appliance is: \"Fridge\", \"WashingMachine\", \"DishWasher\"\n","# The files are: AMPds2_{residence}_{appliance}.csv\n","#\n","# For these files, the columns to use are \"unix_ts\" and the second is \"P\"\n","#\n","# Change the column names to: \"unix_ts\" is \"timestamp\" and \"P\" is \"active_power\"\n","#\n","# Aggregate \"active_power\" for every 1 minute.\n","# Get the distribution of this \"active_power\" and zero the bottom 90th percentile.\n","# Next, please add the \"active_power\" for every 15 minutes.\n","#\n","# Save only \"timestamp\" and \"P\" to: /content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/AMPds2_{residence}_{appliance}_15minutes.csv\n","#\n","# For each {residence}: please print the date range: \"First Date\" and the \"Last Date\"\n","#\n","\n","import pandas as pd\n","import glob\n","import os\n","\n","# ----------------------------\n","# Paths & Config\n","# ----------------------------\n","input_dir = \"/content/drive/MyDrive/Paper02_14Datasets/AMPds2_ORIGINALS\"\n","output_dir = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","residence = \"House01\"\n","appliances = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","\n","# ----------------------------\n","# Loop each appliance\n","# ----------------------------\n","for appliance in appliances:\n","    file_path = f\"{input_dir}/AMPds2_{residence}_{appliance}.csv\"\n","\n","    # --- Load data ---\n","    df = pd.read_csv(file_path)\n","\n","    # Use only unix_ts and P, rename columns\n","    df = df[[\"unix_ts\", \"P\"]].rename(columns={\"unix_ts\": \"timestamp\", \"P\": \"active_power\"})\n","\n","    # Convert timestamp to datetime\n","    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n","    df = df.set_index(\"timestamp\").sort_index()\n","\n","    # ----------------------------\n","    # For each appliance in the residence, determine the 15 minute active power\n","    # ----------------------------\n","    # --- Step 1: resample to 1-minute sums ---\n","    df_1min = df.resample(\"1min\").sum()\n","\n","    # --- Step 2: zero out bottom 90th percentile of active_power distribution ---\n","    thresh = df_1min[\"active_power\"].quantile(0.90)\n","    df_1min.loc[df_1min[\"active_power\"] < thresh, \"active_power\"] = 0\n","\n","    # --- Step 3: resample to 15-minute sums ---\n","    df_15min = df_1min.resample(\"15min\").sum()\n","\n","    # --- Save only timestamp and active_power ---\n","    out_path = f\"{output_dir}/AMPds2_{residence}_{appliance}_15minutes.csv\"\n","    df_15min.reset_index()[[\"timestamp\", \"active_power\"]].to_csv(out_path, index=False)\n","\n","    # --- Print date range ---\n","    first_date = df_15min.index.min()\n","    last_date = df_15min.index.max()\n","    print(f\"{residence} - {appliance}: First Date = {first_date}, Last Date = {last_date}\")\n"],"metadata":{"id":"STHYLzb__lTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cFSgUQcZzSL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### GREEND Dataset - Create Dataset and combine for 15 minutes ####\n","#\n","# GREEND dataset: https://www.kaggle.com/datasets/p111110/greend-energy-dataset\n","#\n","# The files are: /content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS/{residence}/dataset_{year}-{month}-{day}.csv\n","# residence is: \"House00\", \"House01\", \"House03\"\n","# appliance is: \"Fridge\", \"WashingMachine\", \"Microwave\"\n","# year is from \"2014\", \"2015\"\n","# month is from \"01\" to \"12\"\n","# day is from \"01\" to \"31\"\n","#\n","# merge these files together in order of dates ({year}-{month}-{day})\n","#\n","# For \"House00\":\n","# keep only the columns: \"timestamp\", \"000D6F0002907C89\" renamed as \"Fridge\", \"000D6F0002907BC8\" renamed as \"WashingMachine\", \"000D6F0002908150\", renamed as \"Dishwasher\"\n","#\n","# For \"House01\":\n","# keep only the columns: \"timestamp\", \"000D6F00036BB04C\" renamed as \"Fridge\", \"000D6F0003562C48\" renamed as \"WashingMachine\", \"000D6F00029C2BD7\", renamed as \"Dishwasher\"\n","#\n","# For \"House03\":\n","# keep only the columns: \"timestamp\", \"000D6F000356174D\" renamed as \"Fridge\", \"000D6F0003561FFD\" renamed as \"WashingMachine\", \"000D6F0003561747\", renamed as \"Dishwasher\"\n","#\n","# create \"/content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS/GREEND_House01_Fridge.csv\"\n","# with only \"timestamp\" and \"Fridge\"\n","# rename \"Fridge\" to \"active_power\"\n","#\n","# create \"/content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS/GREEND_House01_WashingMachine.csv\"\n","# with only \"timestamp\" and \"WashingMachine\"\n","# rename \"WashingMachine\" to \"active_power\"\n","#\n","# create \"/content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS/GREEND_House01_Dishwasher.csv\"\n","# with only \"timestamp\" and \"Dishwasher\"\n","# rename \"Dishwasher\" to \"active_power\"\n","#\n","# Now, for each of these file: \"/content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS/GREEND_{residence}_{appliance}.csv\"\n","#\n","# Aggregate \"active_power\" for every 1 minute.\n","# Get the distribution of this \"active_power\" and zero the bottom 90th percentile.\n","# Next, please add the \"active_power\" for every 15 minutes.\n","#\n","# Save only \"timestamp\" and \"active_power\" to: /content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/GREEND_{residence}_{appliance}_15minutes.csv\n","#\n","# For each {residence}: please print the date range: \"First Date\" and the \"Last Date\"\n","#\n","\n","import os, glob\n","import pandas as pd\n","\n","# ----------------------------\n","# Paths & Config\n","# ----------------------------\n","BASE = \"/content/drive/MyDrive/Paper02_14Datasets/GREEND_ORIGINALS\"\n","OUT15 = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES\"\n","os.makedirs(OUT15, exist_ok=True)\n","\n","ID_MAPS = {\n","    \"House00\": {\"000D6F0002907C89\": \"Fridge\", \"000D6F0002907BC8\": \"WashingMachine\", \"000D6F0002908150\": \"Dishwasher\"},\n","    \"House01\": {\"000D6F00036BB04C\": \"Fridge\", \"000D6F0003562C48\": \"WashingMachine\", \"000D6F00029C2BD7\": \"Dishwasher\"},\n","    \"House03\": {\"000D6F000356174D\": \"Fridge\", \"000D6F0003561FFD\": \"WashingMachine\", \"000D6F0003561747\": \"Dishwasher\"},\n","}\n","RESIDENCES = [\"House00\", \"House01\", \"House03\"]\n","\n","# ----------------------------\n","# Timestamp parsing: seconds->ms\n","# ----------------------------\n","def parse_timestamp(s: pd.Series) -> pd.Series:\n","    # Cast to numeric first to avoid FutureWarning and support \"1386374400\"-style strings\n","    s_num = pd.to_numeric(s, errors=\"coerce\")\n","    ts = pd.to_datetime(s_num, unit=\"s\", errors=\"coerce\")\n","    if ts.isna().all():\n","        ts = pd.to_datetime(s_num, unit=\"ms\", errors=\"coerce\")\n","    if ts.isna().all():\n","        ts = pd.to_datetime(s, errors=\"coerce\")\n","    return ts\n","\n","# ----------------------------\n","# Loads CSV flexibly, cleans headers, ignores broken rows.\n","# ----------------------------\n","def read_csv_relaxed(fp: str) -> pd.DataFrame | None:\n","    try:\n","        df = pd.read_csv(\n","            fp,\n","            sep=None, engine=\"python\",      # autodetect delimiter\n","            on_bad_lines=\"skip\",\n","            encoding=\"utf-8\", encoding_errors=\"replace\",\n","        )\n","        # normalize column names (strip spaces/quotes)\n","        df.columns = df.columns.astype(str).str.strip().str.replace('\"', '', regex=False)\n","        return df\n","    except Exception as e:\n","        print(f\"  ! Skipping file (cannot parse): {fp} ({e})\")\n","        return None\n","\n","# ----------------------------\n","# Loops through residences\n","#\n","# For each residence, it loads many daily CSV files and keeps only the needed columns.\n","# merges them into one clean time-ordered table,\n","# renames plug IDs to appliance names, and forces appliance values to be numeric.\n","# ----------------------------\n","for res in RESIDENCES:\n","    id_map = ID_MAPS[res]\n","    want_cols = [\"timestamp\"] + list(id_map.keys())\n","\n","    # 1) Collect & sort daily files (YYYY-MM-DD order via filename)\n","    files = sorted(glob.glob(f\"{BASE}/{res}/dataset_*.csv\"))\n","    if not files:\n","        print(f\"[{res}] No files found.\")\n","        continue\n","\n","    merged = []\n","    for fp in files:\n","        df = read_csv_relaxed(fp)\n","        if df is None:\n","            continue\n","\n","        # keep only needed columns that exist\n","        keep = [c for c in want_cols if c in df.columns]\n","        if \"timestamp\" not in keep:\n","            # try to find timestamp by case-insensitive match\n","            maybe_ts = [c for c in df.columns if c.lower().strip() == \"timestamp\"]\n","            if maybe_ts:\n","                df = df.rename(columns={maybe_ts[0]: \"timestamp\"})\n","                keep = [\"timestamp\"] + [c for c in id_map.keys() if c in df.columns]\n","            else:\n","                continue\n","\n","        df = df[keep]\n","        merged.append(df)\n","\n","    if not merged:\n","        print(f\"[{res}] No usable rows.\")\n","        continue\n","\n","    df = pd.concat(merged, ignore_index=True)\n","\n","    # 2) Parse timestamp, sort\n","    df[\"timestamp\"] = parse_timestamp(df[\"timestamp\"])\n","    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n","\n","    # 3) Rename plug IDs → appliance names\n","    rename_map = {k: v for k, v in id_map.items() if k in df.columns}\n","    df = df.rename(columns=rename_map)\n","\n","    # Force appliance columns to numeric\n","    for col in rename_map.values():\n","        if col in df.columns:\n","            df[col] = df[col].astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n","            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n","\n","    # ----------------------------\n","    # For each appliance in the residence, determine the 15 minute active power\n","    # ----------------------------\n","    for appliance in rename_map.values():\n","        if appliance not in df.columns:\n","            continue\n","\n","        # (1) Write originals\n","        out_orig = f\"{BASE}/GREEND_{res}_{appliance}.csv\"\n","        df_orig = df[[\"timestamp\", appliance]].rename(columns={appliance: \"active_power\"})\n","        df_orig.to_csv(out_orig, index=False)\n","\n","        # (2) 1-min sum\n","        ser = df_orig.set_index(\"timestamp\")[\"active_power\"]\n","        ser = pd.to_numeric(ser, errors=\"coerce\")\n","        s_1m = ser.resample(\"1min\").sum(min_count=1)\n","\n","        # (3) Zero bottom 90% (keep top 10%)\n","        if s_1m.notna().any():\n","            thr = s_1m.quantile(0.9)\n","            if pd.isna(thr):\n","                thr = 0.0\n","        else:\n","            thr = 0.0\n","        s_1m = s_1m.fillna(0)\n","        s_1m = s_1m.where(s_1m >= float(thr), 0.0)\n","\n","        # (4) 15-min sum and save\n","        s_15m = s_1m.resample(\"15min\").sum().reset_index()\n","        s_15m.columns = [\"timestamp\", \"active_power\"]\n","\n","        out_15 = f\"{OUT15}/GREEND_{res}_{appliance}_15minutes.csv\"\n","        s_15m.to_csv(out_15, index=False)\n","\n","    # 5) Print date range per residence\n","    first, last = df[\"timestamp\"].min(), df[\"timestamp\"].max()\n","    print(f\"{res}: First Date = {first}, Last Date = {last}\")\n","\n"],"metadata":{"id":"SNWlcwxfz5IY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bA7CHa75F3_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### CENTROIDS  - Average Power Consumption for appliances within each dataset-house combination ####\n","#\n","# Calculates the average active power consumption for specified appliances\n","# within each provided dataset_house combination, and saves the results\n","# to new CSV files.\n","#\n","# The appliances are: Fridge, WashingMachine, Dishwasher, and Nothing\n","#\n","#\n","\n","import pandas as pd\n","import os\n","\n","def calculate_appliance_centroids(combinations, appliances):\n","    print(\"Starting calculation of appliance centroids for specified combinations...\")\n","\n","    # Iterate residence combination\n","    for dataset, house in combinations:\n","        print(f\"\\nProcessing {dataset} - {house}...\")\n","        # List to store centroid data for the current dataset_house combination\n","        centroids_data = []\n","\n","        # Iterate through each appliance\n","        for appliance in appliances:\n","            # Construct the input filename with the \"_15minutes\" suffix\n","            input_filename = f\"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/{dataset}_{house}_{appliance}_15minutes.csv\"\n","\n","            # Check if the input file exists before attempting to read it\n","            if os.path.exists(input_filename):\n","                print(f\"  Reading {input_filename}...\")\n","                try:\n","                    # Read the CSV file into a pandas DataFrame\n","                    # Assuming 'active_power' is a column in the CSV\n","                    df = pd.read_csv(input_filename)\n","\n","                    # Ensure 'active_power' column exists\n","                    if 'active_power' in df.columns:\n","                        # Calculate the average active_power consumption for the appliance\n","                        average_active_power = df['active_power'].mean()\n","                        print(f\"    Average active_power for {appliance}: {average_active_power:.2f}\")\n","\n","                        # Add the appliance and its average consumption to our list\n","                        centroids_data.append({\n","                            'combination': appliance,\n","                            'active_power': average_active_power\n","                        })\n","                    else:\n","                        print(f\"    Warning: 'active_power' column not found in {input_filename}. Skipping.\")\n","                except pd.errors.EmptyDataError:\n","                    print(f\"    Warning: {input_filename} is empty. Skipping.\")\n","                except Exception as e:\n","                    print(f\"    Error reading {input_filename}: {e}. Skipping.\")\n","            else:\n","                print(f\"  File not found: {input_filename}. Skipping.\")\n","\n","        # --- Add 'Nothing' entry with active_power of 0 ---\n","        centroids_data.append({\n","            'combination': 'Nothing',\n","            'active_power': 0.0\n","        })\n","        print(f\"  Added 'Nothing' entry with active_power: 0.0\")\n","\n","        # After processing all appliances for the current house and dataset,\n","        # create a DataFrame from the collected centroid data\n","        if centroids_data:\n","            centroids_df = pd.DataFrame(centroids_data)\n","\n","            # Construct the output filename\n","            output_filename = f\"/content/drive/MyDrive/Paper02_14Datasets/CENTROIDS/{dataset}_{house}_centroids.csv\"\n","\n","            # Save the DataFrame to a new CSV file\n","            centroids_df.to_csv(output_filename, index=False)\n","            print(f\"Successfully created {output_filename}\")\n","        else:\n","            print(f\"No valid appliance data found for {dataset} - {house}. No centroid file created.\")\n","\n","    print(\"\\nCentroid calculation complete.\")\n","\n","if __name__ == \"__main__\":\n","    # --- Configuration ---\n","    # Define the specific dataset and house combinations to process\n","    VALID_COMBINATIONS = [\n","          (\"REFIT\", \"House01\"),\n","          (\"REFIT\", \"House02\"),\n","          (\"REFIT\", \"House03\"),\n","          (\"REFIT\", \"House05\"),\n","          (\"REFIT\", \"House07\"),\n","          (\"REFIT\", \"House09\"),\n","          (\"REFIT\", \"House15\"),\n","          (\"UKDALE\", \"House01\"),\n","          (\"UKDALE\", \"House02\"),\n","          (\"UKDALE\", \"House05\"),\n","          (\"AMPds2\", \"House01\"),\n","          (\"GREEND\", \"House00\"),\n","          (\"GREEND\", \"House01\"),\n","          (\"GREEND\", \"House03\")\n","    ]\n","    APPLIANCES = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","\n","    # Call the main function to perform the calculation and file creation\n","    calculate_appliance_centroids(VALID_COMBINATIONS, APPLIANCES)\n","\n"],"metadata":{"id":"Udpj8zfDF39J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"djQ1QBTmHSie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### ANOMALIES - Create anomalies (7 types for 7 days) plus ground_truth_anomaly ####\n","#\n","#\n","# Anomalies are: StepChange, MultiStepChange, Mirror, Repeating, StuckMAX, StuckMIN, PowerCycling\n","# Use percentiles for values\n","#\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","\n","# ----------------------------\n","# Paths & Config\n","# ----------------------------\n","BASE_INPUT_DIR = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/\"\n","BASE_OUTPUT_ANOMALOUS_DIR = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/anomalous_data/\"\n","BASE_OUTPUT_PLOTS_DIR = \"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/anomaly_plots/\"  # Plots displayed but not saved here\n","\n","os.makedirs(BASE_OUTPUT_ANOMALOUS_DIR, exist_ok=True)\n","os.makedirs(BASE_OUTPUT_PLOTS_DIR, exist_ok=True)\n","\n","datasets = [\"REFIT\", \"UKDALE\", \"AMPds2\", \"GREEND\"]\n","\n","refitt_houses = [\"House01\", \"House02\", \"House03\", \"House05\", \"House07\", \"House09\", \"House15\"]\n","ukdale_houses = [\"House01\", \"House02\", \"House05\"]\n","ampds2_houses = [\"House01\"]\n","greend_houses = [\"House00\", \"House01\", \"House03\"]\n","\n","appliances = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","\n","anomaly_types = [\n","    \"StepChange\",\n","    \"MultiStepChange\",\n","    \"Mirror\",\n","    \"Repeating\",\n","    \"StuckMAX\",\n","    \"StuckMIN\",\n","    \"PowerCycling\"\n","]\n","\n","def get_anomaly_dates(dataset, house):\n","    \"\"\"Returns a list of anomaly dates for a given dataset and house.\"\"\"\n","    if dataset == \"REFIT\" and house == \"House01\":\n","        return [\n","            pd.to_datetime(\"2015-03-09\"), # Monday\n","            pd.to_datetime(\"2015-03-24\"), # Tuesday\n","            pd.to_datetime(\"2015-04-15\"), # Wednesday\n","            pd.to_datetime(\"2015-05-07\"), # Thursday\n","            pd.to_datetime(\"2015-05-29\"), # Friday\n","            pd.to_datetime(\"2015-06-20\"), # Saturday\n","            pd.to_datetime(\"2015-07-05\"), # Sunday\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House02\":\n","        return [\n","            pd.to_datetime(\"2015-01-26\"),\n","            pd.to_datetime(\"2015-02-17\"),\n","            pd.to_datetime(\"2015-03-04\"),\n","            pd.to_datetime(\"2015-03-26\"),\n","            pd.to_datetime(\"2015-04-17\"),\n","            pd.to_datetime(\"2015-05-09\"),\n","            pd.to_datetime(\"2015-05-24\"),\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House03\":\n","        return [\n","            pd.to_datetime(\"2015-02-02\"),\n","            pd.to_datetime(\"2015-02-17\"),\n","            pd.to_datetime(\"2015-03-11\"),\n","            pd.to_datetime(\"2015-04-02\"),\n","            pd.to_datetime(\"2015-04-24\"),\n","            pd.to_datetime(\"2015-05-16\"),\n","            pd.to_datetime(\"2015-05-31\"),\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House05\":\n","        return [\n","            pd.to_datetime(\"2015-03-02\"),\n","            pd.to_datetime(\"2015-03-24\"),\n","            pd.to_datetime(\"2015-04-08\"),\n","            pd.to_datetime(\"2015-04-30\"),\n","            pd.to_datetime(\"2015-05-22\"),\n","            pd.to_datetime(\"2015-06-13\"),\n","            pd.to_datetime(\"2015-07-05\"),\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House07\":\n","        return [\n","            pd.to_datetime(\"2015-03-09\"),\n","            pd.to_datetime(\"2015-03-31\"),\n","            pd.to_datetime(\"2015-04-15\"),\n","            pd.to_datetime(\"2015-05-07\"),\n","            pd.to_datetime(\"2015-05-29\"),\n","            pd.to_datetime(\"2015-06-20\"),\n","            pd.to_datetime(\"2015-07-05\"),\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House09\":\n","        return [\n","            pd.to_datetime(\"2015-03-23\"),\n","            pd.to_datetime(\"2015-04-07\"),\n","            pd.to_datetime(\"2015-04-22\"),\n","            pd.to_datetime(\"2015-05-14\"),\n","            pd.to_datetime(\"2015-05-29\"),\n","            pd.to_datetime(\"2015-06-20\"),\n","            pd.to_datetime(\"2015-07-05\"),\n","        ]\n","    elif dataset == \"REFIT\" and house == \"House15\":\n","        return [\n","            pd.to_datetime(\"2015-03-16\"),\n","            pd.to_datetime(\"2015-04-07\"),\n","            pd.to_datetime(\"2015-04-22\"),\n","            pd.to_datetime(\"2015-05-14\"),\n","            pd.to_datetime(\"2015-05-29\"),\n","            pd.to_datetime(\"2015-06-20\"),\n","            pd.to_datetime(\"2015-07-05\"),\n","        ]\n","    elif dataset == \"UKDALE\" and house == \"House01\":\n","        return [\n","            pd.to_datetime(\"2016-06-06\"),\n","            pd.to_datetime(\"2016-07-26\"),\n","            pd.to_datetime(\"2016-09-21\"),\n","            pd.to_datetime(\"2016-11-17\"),\n","            pd.to_datetime(\"2017-01-06\"),\n","            pd.to_datetime(\"2017-03-04\"),\n","            pd.to_datetime(\"2017-04-23\"),\n","        ]\n","    elif dataset == \"UKDALE\" and house == \"House02\":\n","        return [\n","            pd.to_datetime(\"2013-09-16\"),\n","            pd.to_datetime(\"2013-09-17\"),\n","            pd.to_datetime(\"2013-09-18\"),\n","            pd.to_datetime(\"2013-09-26\"),\n","            pd.to_datetime(\"2013-10-04\"),\n","            pd.to_datetime(\"2013-10-05\"),\n","            pd.to_datetime(\"2013-10-06\"),\n","        ]\n","    elif dataset == \"UKDALE\" and house == \"House05\":\n","        return [\n","            pd.to_datetime(\"2014-10-20\"),\n","            pd.to_datetime(\"2014-10-21\"),\n","            pd.to_datetime(\"2014-10-29\"),\n","            pd.to_datetime(\"2014-10-30\"),\n","            pd.to_datetime(\"2014-11-07\"),\n","            pd.to_datetime(\"2014-11-08\"),\n","            pd.to_datetime(\"2014-11-09\"),\n","        ]\n","    elif dataset == \"AMPds2\" and house == \"House01\":\n","        return [\n","            pd.to_datetime(\"2013-11-11\"),\n","            pd.to_datetime(\"2013-12-03\"),\n","            pd.to_datetime(\"2013-12-25\"),\n","            pd.to_datetime(\"2014-01-16\"),\n","            pd.to_datetime(\"2014-02-14\"),\n","            pd.to_datetime(\"2014-03-08\"),\n","            pd.to_datetime(\"2014-03-30\"),\n","        ]\n","    elif dataset == \"GREEND\" and house == \"House00\":\n","        return [\n","            pd.to_datetime(\"2014-08-18\"),\n","            pd.to_datetime(\"2014-08-26\"),\n","            pd.to_datetime(\"2014-09-03\"),\n","            pd.to_datetime(\"2014-09-11\"),\n","            pd.to_datetime(\"2014-09-26\"),\n","            pd.to_datetime(\"2014-10-04\"),\n","            pd.to_datetime(\"2014-10-12\"),\n","        ]\n","    elif dataset == \"GREEND\" and house == \"House01\":\n","        return [\n","            pd.to_datetime(\"2014-09-15\"),\n","            pd.to_datetime(\"2014-09-23\"),\n","            pd.to_datetime(\"2014-10-01\"),\n","            pd.to_datetime(\"2014-10-09\"),\n","            pd.to_datetime(\"2014-10-17\"),\n","            pd.to_datetime(\"2014-10-25\"),\n","            pd.to_datetime(\"2014-10-26\"),\n","        ]\n","    elif dataset == \"GREEND\" and house == \"House03\":\n","        return [\n","            pd.to_datetime(\"2014-09-22\"),\n","            pd.to_datetime(\"2014-09-23\"),\n","            pd.to_datetime(\"2014-10-01\"),\n","            pd.to_datetime(\"2014-10-09\"),\n","            pd.to_datetime(\"2014-10-17\"),\n","            pd.to_datetime(\"2014-10-25\"),\n","            pd.to_datetime(\"2014-10-26\"),\n","        ]\n","    return []\n","\n","# ----------------------------\n","# Anomaly Generation Helpers\n","# ----------------------------\n","# Given a list, determine the value for a specified percentile.\n","def get_percentile_value(ap_list, percentile):\n","    if not ap_list:\n","        return 0\n","    index = int(len(ap_list) * percentile / 100)\n","    return ap_list[min(index, len(ap_list) - 1)]\n","\n","# Determine if a day is flat by standard deviation of active_power being less than a threshold\n","# A standard deviation less than 1 means the values are tightly clustered around the average, with very little variation from it.\n","def is_day_flat(df_day_segment, threshold=1.0):\n","    if df_day_segment.empty or len(df_day_segment) < 2:\n","        return True\n","    return np.std(df_day_segment['active_power'].values) < threshold\n","\n","# Searches backwards from given day, for the nearest non-flat day.\n","def find_non_flat_day_data(df_full, current_anomaly_date, threshold=1.0):\n","    df_full_indexed = df_full.copy().set_index('timestamp').sort_index()\n","    search_date = current_anomaly_date - pd.Timedelta(days=1)\n","\n","    while search_date >= df_full_indexed.index.min().floor('D'):\n","        day_start = search_date.floor('D')\n","        day_end = search_date.floor('D') + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n","        df_day_candidate = df_full_indexed.loc[day_start:day_end].copy()\n","\n","        if not df_day_candidate.empty and not is_day_flat(df_day_candidate, threshold):\n","            print(f\"Found non-flat day for pattern: {day_start.date()}\")\n","            return df_day_candidate[['active_power']]\n","\n","        search_date -= pd.Timedelta(days=1)\n","\n","    print(f\"Warning: No non-flat day found in historical data for date {current_anomaly_date.date()} for this appliance.\")\n","    return pd.DataFrame(columns=['active_power'], index=pd.to_datetime([]))\n","\n","# ----------------------------\n","# Anomaly Definitions\n","# ----------------------------\n","# StepChange: first half 15th percentile, second half 98th percentile\n","def apply_step_change(df_day, ap_list):\n","    df_anomalous = df_day.copy()\n","    if len(ap_list) == 0:\n","        print(\"Warning: active_power_list is empty. Cannot apply step change.\")\n","        return df_anomalous\n","\n","    percentile_15 = get_percentile_value(ap_list, 15)\n","    percentile_98 = get_percentile_value(ap_list, 98)\n","\n","    midpoint_idx = len(df_anomalous) // 2\n","    df_anomalous.iloc[:midpoint_idx, df_anomalous.columns.get_loc('active_power')] = percentile_15\n","    df_anomalous.iloc[midpoint_idx:, df_anomalous.columns.get_loc('active_power')] = percentile_98\n","    return df_anomalous\n","\n","# MultiStepChange: first third 15th, middle third 75th, last third 98th percentile\n","def apply_multi_step_change(df_day, ap_list):\n","    df_anomalous = df_day.copy()\n","    if len(ap_list) == 0:\n","        print(\"Warning: active_power_list is empty. Cannot apply multi-step change.\")\n","        return df_anomalous\n","\n","    percentile_15 = get_percentile_value(ap_list, 15)\n","    percentile_75 = get_percentile_value(ap_list, 75)\n","    percentile_98 = get_percentile_value(ap_list, 98)\n","\n","    len_df = len(df_anomalous)\n","    first_third_end = len_df // 3\n","    second_third_end = 2 * len_df // 3\n","\n","    df_anomalous.iloc[:first_third_end, df_anomalous.columns.get_loc('active_power')] = percentile_15\n","    df_anomalous.iloc[first_third_end:second_third_end, df_anomalous.columns.get_loc('active_power')] = percentile_75\n","    df_anomalous.iloc[second_third_end:, df_anomalous.columns.get_loc('active_power')] = percentile_98\n","    return df_anomalous\n","\n","# Mirror:\n","# Flips active power horizontally.\n","# If current day is flat, uses provided non-flat source data (tiled if needed).\n","# Ensures minimum value >= 15th percentile.\n","def apply_mirror(df_day, ap_list, df_source_for_pattern):\n","    \"\"\"\n","    Applies a mirror anomaly:\n","    \"\"\"\n","    df_anomalous = df_day.copy()\n","    source_power_values = df_anomalous['active_power'].values\n","\n","    if is_day_flat(df_anomalous) and not df_source_for_pattern.empty:\n","        print(f\"Using non-flat source data for Mirror anomaly on {df_day.index[0].date()}\")\n","        if len(df_source_for_pattern) >= len(df_anomalous):\n","            source_power_values = df_source_for_pattern['active_power'].values[:len(df_anomalous)]\n","        else:\n","            source_power_values = np.tile(\n","                df_source_for_pattern['active_power'].values,\n","                int(np.ceil(len(df_anomalous) / len(df_source_for_pattern)))\n","            )[:len(df_anomalous)]\n","    elif is_day_flat(df_anomalous) and df_source_for_pattern.empty:\n","        print(f\"Mirror anomaly on {df_day.index[0].date()}: Current day is flat and no non-flat historical data found. Applying horizontal flip to current flat data.\")\n","    else:\n","        print(f\"Applying Mirror anomaly to non-flat current day data on {df_day.index[0].date()}\")\n","\n","    mirrored_values = source_power_values[::-1]\n","\n","    if ap_list:\n","        percentile_15 = get_percentile_value(ap_list, 15)\n","        mirrored_values[mirrored_values < percentile_15] = percentile_15\n","        print(f\"Ensured all mirrored values are at least {percentile_15:.2f} (15th percentile).\")\n","\n","    df_anomalous.loc[:, 'active_power'] = mirrored_values\n","    return df_anomalous\n","\n","# Repeating:\n","# Finds a 'bump' (max avg 2-hour window) and repeats it 6 times.\n","# If no bump available, uses source or simple [15th, 98th] pattern.\n","# Ensures minimum value >= 15th percentile.\n","def apply_repeating(df_day, ap_list, df_source_for_pattern):\n","    # Work on a copy so original data is not modified\n","    df_anomalous = df_day.copy()\n","\n","    # Guard: empty day → nothing to modify\n","    if len(df_anomalous) == 0:\n","        print(\"Warning: df_anomalous is empty. Cannot apply repeating anomaly.\")\n","        return df_anomalous\n","\n","    # Total number of time intervals in the day (96 for 15-min data)\n","    total_intervals = len(df_anomalous)\n","\n","    # Number of repeated blocks to insert across the day\n","    num_repetitions = 6\n","    if num_repetitions == 0:\n","        print(\"Warning: Number of repetitions is 0. Cannot apply repeating anomaly.\")\n","        return df_anomalous\n","\n","    # Length of each repeated segment\n","    segment_length = total_intervals // num_repetitions\n","    if segment_length == 0:\n","        # Day too short → fallback to simple alternating low/high pattern\n","        print(\"Warning: Segment length is 0. Cannot repeat the bump. Day too short or too many repetitions.\")\n","        df_anomalous.loc[:, 'active_power'] = np.array(\n","            [get_percentile_value(ap_list, 15), get_percentile_value(ap_list, 98)]\n","        )[np.arange(total_intervals) % 2]\n","        return df_anomalous\n","\n","    # Maximum duration of the repeating “bump” (2 hours = 8 × 15-min)\n","    bump_duration_intervals = min(8, segment_length)\n","\n","    # Default: search for bump pattern in the current day\n","    df_for_bump_search = df_anomalous\n","\n","    # If current day is flat, try to borrow a pattern from historical data\n","    if is_day_flat(df_anomalous) and not df_source_for_pattern.empty:\n","        print(f\"Using non-flat source data for Repeating anomaly on {df_day.index[0].date()}\")\n","        df_for_bump_search = df_source_for_pattern.copy()\n","    elif is_day_flat(df_anomalous) and df_source_for_pattern.empty:\n","        print(f\"Repeating anomaly on {df_day.index[0].date()}: Current day is flat and no non-flat historical data found. Falling back to simple pattern.\")\n","    else:\n","        print(f\"Applying Repeating anomaly to non-flat current day data on {df_day.index[0].date()}\")\n","\n","    # Find the highest-power contiguous window (best “bump”)\n","    max_avg_power = -1\n","    best_bump_start_idx = 0\n","\n","    if not df_for_bump_search.empty and len(df_for_bump_search) >= bump_duration_intervals:\n","        # Slide a window and pick the segment with the largest mean power\n","        for i in range(len(df_for_bump_search) - bump_duration_intervals + 1):\n","            current_avg_power = df_for_bump_search.iloc[\n","                i : i + bump_duration_intervals,\n","                df_for_bump_search.columns.get_loc('active_power')\n","            ].mean()\n","            if current_avg_power > max_avg_power:\n","                max_avg_power = current_avg_power\n","                best_bump_start_idx = i\n","\n","        # Extract the selected bump pattern\n","        bump_pattern = df_for_bump_search.iloc[\n","            best_bump_start_idx : best_bump_start_idx + bump_duration_intervals,\n","            df_for_bump_search.columns.get_loc('active_power')\n","        ].values\n","\n","    elif not df_for_bump_search.empty:\n","        # Fallback: use entire available signal as pattern\n","        bump_pattern = df_for_bump_search['active_power'].values\n","    else:\n","        bump_pattern = np.array([])\n","\n","    # Final fallback if no pattern was found\n","    if len(bump_pattern) == 0:\n","        print(\"Warning: No suitable bump pattern found (even with fallback). Using simple on-off pattern.\")\n","        bump_pattern = np.array([\n","            get_percentile_value(ap_list, 15),\n","            get_percentile_value(ap_list, 98)\n","        ])\n","\n","    # Build one repetition block by cycling through bump values\n","    repetition_block = np.zeros(segment_length)\n","    for i in range(segment_length):\n","        repetition_block[i] = bump_pattern[i % len(bump_pattern)]\n","\n","    # Enforce minimum power floor using 15th percentile\n","    if ap_list and len(repetition_block) > 0:\n","        percentile_15 = get_percentile_value(ap_list, 15)\n","        repetition_block[repetition_block < percentile_15] = percentile_15\n","        print(f\"Ensured all values in repetition block are at least {percentile_15:.2f} (15th percentile).\")\n","\n","    # Tile repetition blocks across the entire day\n","    repeated_values = np.tile(repetition_block, num_repetitions)\n","\n","    # Handle leftover or excess intervals\n","    if len(repeated_values) < total_intervals:\n","        remainder_len = total_intervals - len(repeated_values)\n","        repeated_values = np.append(repeated_values, repetition_block[:remainder_len])\n","    elif len(repeated_values) > total_intervals:\n","        repeated_values = repeated_values[:total_intervals]\n","\n","    # Overwrite active power with repeating anomaly\n","    df_anomalous.loc[:, 'active_power'] = repeated_values\n","\n","    return df_anomalous\n","\n","# StuckMAX: sets active power to 98th percentile\n","def apply_stuck_max(df_day, ap_list):\n","    df_anomalous = df_day.copy()\n","    if len(ap_list) == 0:\n","        print(\"Warning: active_power_list is empty. Cannot apply stuck max.\")\n","        return df_anomalous\n","    percentile_98 = get_percentile_value(ap_list, 98)\n","    df_anomalous.loc[:, 'active_power'] = percentile_98\n","    return df_anomalous\n","\n","# StuckMIN: sets active power to 15th percentile.\n","def apply_stuck_min(df_day, ap_list):\n","    df_anomalous = df_day.copy()\n","    if len(ap_list) == 0:\n","        print(\"Warning: active_power_list is empty. Cannot apply stuck min.\")\n","        return df_anomalous\n","    percentile_15 = get_percentile_value(ap_list, 15)\n","    df_anomalous.loc[:, 'active_power'] = percentile_15\n","    return df_anomalous\n","\n","# PowerCycling: alternates between 15th and 98th percentile 10 times.\n","def apply_power_cycling(df_day, ap_list):\n","    df_anomalous = df_day.copy()\n","    if len(ap_list) == 0:\n","        print(\"Warning: active_power_list is empty. Cannot apply power cycling.\")\n","        return df_anomalous\n","\n","    percentile_15 = get_percentile_value(ap_list, 15)\n","    percentile_98 = get_percentile_value(ap_list, 98)\n","\n","    num_cycles = 10\n","    total_segments = num_cycles * 2\n","    if total_segments == 0:\n","        print(\"Warning: Total segments for power cycling is 0. Cannot apply power cycling.\")\n","        return df_anomalous\n","\n","    segment_intervals = round(len(df_anomalous) / total_segments)\n","    if segment_intervals == 0:\n","        print(\"Warning: Segment interval for power cycling is 0. Day too short for 10 cycles.\")\n","        return df_anomalous\n","\n","    current_value = percentile_15\n","    current_segment_count = 0\n","    for i in range(len(df_anomalous)):\n","        df_anomalous.iloc[i, df_anomalous.columns.get_loc('active_power')] = current_value\n","        current_segment_count += 1\n","        if current_segment_count >= segment_intervals:\n","            current_value = percentile_98 if current_value == percentile_15 else percentile_15\n","            current_segment_count = 0\n","\n","    return df_anomalous\n","\n","# ----------------------------\n","# Main Loop\n","# Loop though datasets and houses within the datasets.\n","# ----------------------------\n","for dataset in datasets:\n","    # Determine houses based on dataset\n","    if dataset == \"REFIT\":\n","        current_houses = refitt_houses\n","    elif dataset == \"UKDALE\":\n","        current_houses = ukdale_houses\n","    elif dataset == \"AMPds2\":\n","        current_houses = ampds2_houses\n","    elif dataset == \"GREEND\":\n","        current_houses = greend_houses\n","    else:\n","        continue  # Skip if dataset not defined\n","\n","    for house in current_houses:\n","        for appliance in appliances:\n","            file_path = os.path.join(BASE_INPUT_DIR, f\"{dataset}_{house}_{appliance}_15minutes.csv\")\n","\n","            if not os.path.exists(file_path):\n","                print(f\"File not found: {file_path}. Skipping.\")\n","                continue\n","\n","            print(f\"Processing {dataset}, {house}, {appliance}...\")\n","            df_original_full = pd.read_csv(file_path)\n","            df_original_full['timestamp'] = pd.to_datetime(df_original_full['timestamp'])\n","            # Ensure 'active_power' is numeric\n","            df_original_full['active_power'] = pd.to_numeric(\n","                df_original_full['active_power'], errors='coerce'\n","            ).fillna(0)\n","\n","            # Create sorted list of unique active power values\n","            active_power_list = sorted(df_original_full['active_power'].dropna().unique().tolist())\n","            print(f\"Unique active power values for {appliance} in {house} ({dataset}):\")\n","            print(active_power_list)\n","\n","            # Get anomaly dates for the current dataset and house\n","            anomaly_dates = get_anomaly_dates(dataset, house)\n","\n","            if not anomaly_dates:\n","                print(f\"No anomaly dates defined for {dataset} {house}. Skipping.\")\n","                continue\n","\n","            # Loop through the different anomaly types\n","            for anomaly_type in anomaly_types:\n","                # Initialize df_modified_full and ground_truth_anomaly for each anomaly type\n","                df_modified_full = df_original_full.copy()\n","                df_modified_full['ground_truth_anomaly'] = \"Normal\"\n","\n","                # Loop through the anomaly dates\n","                for anomaly_date in anomaly_dates:\n","                    # Get the anomaly start and end dates (end is one day)\n","                    anomaly_day_start = anomaly_date.floor('D')\n","                    anomaly_day_end = anomaly_date.floor('D') + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n","\n","                    # Original day slice (for plotting)\n","                    df_day_original = df_original_full[\n","                        (df_original_full['timestamp'] >= anomaly_day_start) &\n","                        (df_original_full['timestamp'] <= anomaly_day_end)\n","                    ].copy()\n","\n","                    if df_day_original.empty:\n","                        print(f\"No data for anomaly date {anomaly_date.date()} in {dataset}_{house}_{appliance}. Skipping anomaly generation for this date for {anomaly_type}.\")\n","                        continue\n","\n","                    # Get the date to add the anomaly\n","                    df_day_segment_to_modify = df_modified_full[\n","                        (df_modified_full['timestamp'] >= anomaly_day_start) &\n","                        (df_modified_full['timestamp'] <= anomaly_day_end)\n","                    ].copy()\n","\n","                    if df_day_segment_to_modify.empty:\n","                        print(f\"No modifiable segment found for anomaly date {anomaly_date.date()} in {dataset}_{house}_{appliance}. Cannot apply {anomaly_type}.\")\n","                        continue\n","\n","                    # Find non-flat day for Mirror and Repeating anomalies\n","                    non_flat_source_data = find_non_flat_day_data(df_original_full, anomaly_date)\n","\n","                    # Indices of the anomaly day in the full dataframe\n","                    anomaly_indices = df_modified_full[\n","                        (df_modified_full['timestamp'] >= anomaly_day_start) &\n","                        (df_modified_full['timestamp'] <= anomaly_day_end)\n","                    ].index\n","\n","                    if anomaly_indices.empty:\n","                        print(f\"No indices found for anomaly date {anomaly_date.date()} in {dataset}_{house}_{appliance}. Cannot apply {anomaly_type}.\")\n","                        continue\n","\n","                    # Extract the day's active_power segment\n","                    active_power_day_segment = df_original_full.loc[anomaly_indices, ['timestamp', 'active_power']].copy()\n","                    active_power_day_segment.set_index('timestamp', inplace=True)\n","\n","                    # For each anomaly type, add the anomaly in the range (active_power_day_segment)\n","                    df_anomalous_day_slice = None\n","                    if anomaly_type == \"StepChange\":\n","                        df_anomalous_day_slice = apply_step_change(active_power_day_segment, active_power_list)\n","                    elif anomaly_type == \"MultiStepChange\":\n","                        df_anomalous_day_slice = apply_multi_step_change(active_power_day_segment, active_power_list)\n","                    elif anomaly_type == \"Mirror\":\n","                        df_anomalous_day_slice = apply_mirror(active_power_day_segment, active_power_list, non_flat_source_data)\n","                    elif anomaly_type == \"Repeating\":\n","                        df_anomalous_day_slice = apply_repeating(active_power_day_segment, active_power_list, non_flat_source_data)\n","                    elif anomaly_type == \"StuckMAX\":\n","                        df_anomalous_day_slice = apply_stuck_max(active_power_day_segment, active_power_list)\n","                    elif anomaly_type == \"StuckMIN\":\n","                        df_anomalous_day_slice = apply_stuck_min(active_power_day_segment, active_power_list)\n","                    elif anomaly_type == \"PowerCycling\":\n","                        df_anomalous_day_slice = apply_power_cycling(active_power_day_segment, active_power_list)\n","\n","                    if df_anomalous_day_slice is None:\n","                        print(f\"Anomaly type {anomaly_type} not implemented or failed for {anomaly_date.date()}.\")\n","                        continue\n","\n","                    # Update modified full dataframe\n","                    df_modified_full.loc[anomaly_indices, 'active_power'] = df_anomalous_day_slice['active_power'].values\n","                    df_modified_full.loc[anomaly_indices, 'ground_truth_anomaly'] = \"Anomaly\"\n","\n","                    # Plotting for display only\n","                    plt.figure(figsize=(15, 7))\n","                    plt.plot(df_day_original['timestamp'], df_day_original['active_power'],\n","                             label='Original Active Power', color='blue', alpha=0.7)\n","                    plt.plot(df_anomalous_day_slice.index, df_anomalous_day_slice['active_power'],\n","                             label=f'Anomalous Active Power ({anomaly_type})', color='red', linestyle='--', alpha=0.8)\n","\n","                    plt.title(f\"{dataset} {house} {appliance}: {anomaly_type} Anomaly on {anomaly_date.date()}\")\n","                    plt.xlabel(\"Timestamp\")\n","                    plt.ylabel(\"Active Power\")\n","                    plt.legend()\n","                    plt.grid(True)\n","                    plt.tight_layout()\n","                    plt.show()\n","                    plt.close()\n","                    print(f\"Displayed plot for {dataset}_{house}_{appliance}_{anomaly_type}_{anomaly_date.strftime('%Y-%m-%d')}.\")\n","\n","                # --- Save the data for this specific anomaly type ---\n","                output_anomalous_filename = f\"{dataset}_{house}_{appliance}_15minutes_{anomaly_type}.csv\"\n","                output_anomalous_filepath = os.path.join(BASE_OUTPUT_ANOMALOUS_DIR, output_anomalous_filename)\n","\n","                df_modified_full[['timestamp', 'active_power', 'ground_truth_anomaly']].to_csv(\n","                    output_anomalous_filepath, index=False\n","                )\n","                print(f\"Saved anomalous data for {anomaly_type}: {output_anomalous_filepath}\")\n","\n","print(\"Anomaly generation complete!\")\n"],"metadata":{"id":"1TJgmVn8HSgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sLrexfAmLBE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### MERGE - combines anomalies and normal data plus sets ground_truth_appliance ####\n","#\n","# Loops through specified dataset, house, appliance, and anomaly combinations.\n","# For each anomalous file, it merges the data with normal appliance data from\n","# other appliances in the same house. It then calculates the total active power\n","# and determines the 'ground_truth_appliance' string based on active appliances.\n","# The resulting merged data is saved to a new CSV file.\n","#\n","# Essentially, it uses the current anomaly file (with specified appliance)\n","# and adds the non-anomaly (other appliances).\n","#\n","\n","# Imports\n","import pandas as pd\n","import os\n","\n","def process_anomalous_files():\n","\n","    # ----------------------------\n","    # Paths & Config\n","    # ----------------------------\n","    dataset_list = [\"REFIT\", \"UKDALE\", \"AMPds2\", \"GREEND\"]\n","    house_list = [\"House00\", \"House01\", \"House02\", \"House03\", \"House05\", \"House07\", \"House09\", \"House15\"]\n","    appliance_list = [\"Fridge\", \"WashingMachine\", \"Dishwasher\"]\n","    anomaly_list = [\"StepChange\", \"MultiStepChange\", \"Mirror\", \"Repeating\", \"StuckMAX\", \"StuckMIN\", \"PowerCycling\"]\n","\n","    merged_output_dir = \"/content/drive/MyDrive/Paper02_14Datasets/MERGED\"\n","    os.makedirs(merged_output_dir, exist_ok=True)\n","    print(f\"Ensured output directory exists: {merged_output_dir}\")\n","\n","    # ----------------------------\n","    # Loop through the datasets\n","    # ----------------------------\n","    for dataset in dataset_list:\n","        for house in house_list:\n","            print(f\"\\nProcessing {dataset} - {house}...\")\n","\n","            # Load all normal appliance data for the current dataset and house ---\n","            normal_appliance_data = {}\n","            for appliance in appliance_list:\n","                normal_file_path = f\"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/{dataset}_{house}_{appliance}_15minutes.csv\"\n","                try:\n","                    df = pd.read_csv(normal_file_path)\n","                    # Convert timestamp to datetime objects for accurate merging\n","                    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","                    normal_appliance_data[appliance] = df\n","                    print(f\"  Loaded normal data for {appliance}: {normal_file_path}\")\n","                except FileNotFoundError:\n","                    # If a normal file is not found, print a warning and mark it as None\n","                    print(f\"  Warning: Normal file not found: {normal_file_path}. This appliance's normal data will be considered zero for merges in this house.\")\n","                    normal_appliance_data[appliance] = None\n","                except Exception as e:\n","                    # Catch any other potential errors during file loading\n","                    print(f\"  Error loading normal file {normal_file_path}: {e}\")\n","                    normal_appliance_data[appliance] = None\n","\n","            # Loop through each appliance\n","            for anomaly_appliance in appliance_list:\n","                # Loop through each type of anomaly\n","                for anomaly_type in anomaly_list:\n","                    anomalous_file_path = f\"/content/drive/MyDrive/Paper02_14Datasets/ORIGINALS_15MINUTES/anomalous_data/{dataset}_{house}_{anomaly_appliance}_15minutes_{anomaly_type}.csv\"\n","                    merged_output_file_path = os.path.join(merged_output_dir, f\"{dataset}_{house}_{anomaly_appliance}_15minutes_{anomaly_type}_MERGED.csv\")\n","\n","                    try:\n","                        # Load the specific anomalous appliance data ---\n","                        anomalous_df = pd.read_csv(anomalous_file_path)\n","                        anomalous_df['timestamp'] = pd.to_datetime(anomalous_df['timestamp'])\n","                        if 'ground_truth_anomaly' not in anomalous_df.columns:\n","                            anomalous_df['ground_truth_anomaly'] = True\n","\n","                        print(f\"  Loaded anomalous data: {anomalous_file_path}\")\n","\n","                        # Initialize a base with the above timestamp and ground_truth_anomaly\n","                        base_df = anomalous_df[['timestamp', 'ground_truth_anomaly']].copy()\n","\n","                        # Dictionary to hold the active power series for each appliance, aligned by timestamp\n","                        aligned_power_data = {}\n","\n","                        # Loop through the appliances\n","                        for current_appliance in appliance_list:\n","                            df_to_use = None\n","                            if current_appliance == anomaly_appliance:\n","                                # If current appliance has anomaly, use its anomalous data\n","                                df_to_use = anomalous_df\n","                            elif normal_appliance_data.get(current_appliance) is not None:\n","                                # Otherwise, use its normal data\n","                                df_to_use = normal_appliance_data[current_appliance]\n","                            else:\n","                                # If normal data for this appliance is missing for the current house,\n","                                # assume zero power for its contribution in the merge.\n","                                aligned_power_data[f'active_power_{current_appliance}'] = pd.Series(\n","                                    0.0, index=base_df.index, name=f'active_power_{current_appliance}'\n","                                )\n","                                print(f\"    Note: Using zero power for {current_appliance} in this merge scenario due to missing normal data for {house}.\")\n","                                continue # Move to the next appliance\n","\n","                            # Merge the current appliance's power data with the base_df to align timestamps.\n","                            # Use a temporary column name to prevent conflicts during the merge,\n","                            # then fill any NaN values (where timestamps didn't match) with 0.0.\n","                            temp_col_name = 'temp_active_power'\n","                            # Select only 'timestamp' and 'active_power' from df_to_use for the merge\n","                            merged_temp = pd.merge(\n","                                base_df[['timestamp']], # Merge only on timestamp from base_df\n","                                df_to_use[['timestamp', 'active_power']].rename(columns={'active_power': temp_col_name}),\n","                                on='timestamp',\n","                                how='left' # Left merge keeps all timestamps from base_df\n","                            )\n","                            # Fill any NaN values\n","                            aligned_power_data[f'active_power_{current_appliance}'] = merged_temp[temp_col_name].fillna(0.0)\n","\n","                        # Compute total active power and ground truth appliance string\n","                        final_merged_df = base_df.copy() # Start with timestamps and ground_truth_anomaly\n","\n","                        # Pre-allocate lists to store results for better performance\n","                        total_powers_list = []\n","                        ground_truths_list = []\n","\n","                        # Iterate through each row (timestamp) to calculate total power and ground truth appliance string\n","                        # Using .itertuples() or iterating over the index can be efficient\n","                        for index, row in base_df.iterrows():\n","                            current_total_power = 0.0\n","                            current_active_appliances = []\n","\n","                            # Iterate through appliances in the specified order to build the ground truth string\n","                            for app in appliance_list:\n","                                # Get the aligned power value for the current appliance at this timestamp\n","                                # Use .loc with index for robust lookup\n","                                app_power = aligned_power_data[f'active_power_{app}'].loc[index]\n","                                current_total_power += app_power\n","\n","                                # If appliance's power is above zero, add its name to the active list\n","                                if app_power > 0:\n","                                    current_active_appliances.append(app)\n","\n","                            total_powers_list.append(current_total_power)\n","                            # Form the ground truth string: '+' separated or 'Nothing' if no appliances are active\n","                            ground_truths_list.append('+'.join(current_active_appliances) if current_active_appliances else 'Nothing')\n","\n","                        # Assign the computed lists back to the DataFrame columns\n","                        final_merged_df['active_power'] = total_powers_list\n","                        final_merged_df['ground_truth_appliance'] = ground_truths_list\n","\n","                        # Save 'timestamp', 'active_power', 'ground_truth_anomaly', 'ground_truth_appliance'\n","                        final_output_df = final_merged_df[['timestamp', 'active_power', 'ground_truth_anomaly', 'ground_truth_appliance']]\n","\n","                        # Save the merged DataFrame to the specified path\n","                        final_output_df.to_csv(merged_output_file_path, index=False)\n","                        print(f\"  Successfully created merged file: {merged_output_file_path}\")\n","\n","                    except FileNotFoundError:\n","                        print(f\"  Anomalous file not found: {anomalous_file_path}. Skipping merge for this anomaly combination.\")\n","                    except Exception as e:\n","                        print(f\"  Error processing {anomalous_file_path} for {dataset}_{house}_{anomaly_appliance}_{anomaly_type}: {e}\")\n","\n","# Execute the main function when the script is run\n","if __name__ == \"__main__\":\n","    process_anomalous_files()\n"],"metadata":{"id":"lC7FNiqCLBCF"},"execution_count":null,"outputs":[]}]}